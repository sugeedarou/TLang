{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "qb9qKvDdMbNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation of transformers library\n",
        "! pip install transformers\n"
      ],
      "metadata": {
        "id": "jYNPBqRnEdJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5d5079-25c1-4366-eceb-b0bee33fd759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIFgZcfWlC8F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, DistilBertTokenizer, DistilBertForSequenceClassification,BertTokenizer,BertForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.utils as utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Pfi1QUkuEss5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91af6cd8-e25d-431c-f99e-5641ef5cca13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Working Directory \n",
        "%cd /content/gdrive/MyDrive/Projects/Project_Tweet\n",
        "\n"
      ],
      "metadata": {
        "id": "-xkkoUVsEu_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "Ar3qNZz0MiGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gkv1hk-mr9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "b6fd5620-cbb4-41a7-e6f8-a75e687c66c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  lang                                               text\n",
            "0   ko  teriana Jacobs 누드 및 포르노 유출! https://t.co/lrzcX...\n",
            "1   de  RT @catwiesl: @Ricarda_Lang Warum hofiert ihr ...\n",
            "2   en  RT @fanplus_app: ➖\\nGet 10k VTs, Only for 6 da...\n",
            "3   ja  @ayami_dayoo 残念ながらはずれです…！\\n当たるまで毎日挑戦できます！\\n当選す...\n",
            "4   es  Mi Círculo de Interacción de Twitter\\n\\nGenera...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa6833d5970>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# visualization of our dataset using barchart\n",
        "import pandas as pd\n",
        "df2 = pd.read_csv('final.csv')\n",
        "print(df2.head())\n",
        "dfd = df2.groupby(['lang'])['lang'].count()\n",
        "percentages = dfd.sort_values(ascending=False)/df2.shape[0]\n",
        "percentages.plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv('train.csv')\n",
        "df2 = pd.read_csv('final.csv') # test.csv for testset and final for our dataset\n",
        "dfd = df1.groupby(['lang'])['lang'].count()\n",
        "\n",
        "percentages = dfd.sort_values(ascending=False)/df1.shape[0]\n",
        "percentages.plot.bar(figsize = (12,8))\n",
        "plt.savefig(\"lang.png\")\n",
        "uniform_sampled = df1.groupby([\"lang\"]).sample(n=10, random_state=2, replace=True) # small for debuggig high for training \n",
        "#print(uniform_sampled[\"lang\"].value_counts())\n",
        "# concatenation to do same onehot encoding on two dataframes\n",
        "\n",
        "df = pd.concat([uniform_sampled, df2], axis=0)\n",
        "df_d = df.drop(['id'], axis=1)\n",
        "#print(df_d.head())\n",
        "#print(len(uniform_sampled))\n",
        "# Perform one-hot encoding\n",
        "df_o = pd.get_dummies(df_d,columns=[\"lang\"])\n",
        "#print(df_o.columns)\n",
        "onehot = df_o.iloc[:len(uniform_sampled), :]\n",
        "onehot_t = df_o.iloc[len(uniform_sampled):, :]\n",
        "label_train = onehot.drop([\"text\"], axis=1)\n",
        "label_test = onehot_t.drop([\"text\"], axis=1)\n",
        "\n",
        "\n",
        "# visualization using a piechart \n",
        "\"\"\"plt.style.use('seaborn')\n",
        "label_size = [df1.groupby(['lang'])['lang'].count()]\n",
        "\n",
        "plt.pie(label_size,startangle=90,labels= df1[\"lang\"].unique(),autopct='%1.1f%%')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ShD0gJOYB3MG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "e8d80c27-314b-4d05-a746-82d268a9b0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'plt.style.use(\\'seaborn\\')\\nlabel_size = [df1.groupby([\\'lang\\'])[\\'lang\\'].count()]\\n\\nplt.pie(label_size,startangle=90,labels= df1[\"lang\"].unique(),autopct=\\'%1.1f%%\\')\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "KoR_y6vFIvKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we check the distribution of the length of our training data \n",
        "df[\"len\"] = df['text'].map(lambda x: len(x))\n",
        "df[\"len\"].hist(bins=30)\n",
        "print(df[\"len\"][df[\"len\"]>100].count())\n",
        "print(df[\"len\"][df[\"len\"]>128].count())\n",
        "print(df[\"len\"][df[\"len\"]>140].count())\n",
        "print(df[\"lang\"][df[\"len\"]<2])"
      ],
      "metadata": {
        "id": "olrJhbljv3g4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e86a0a57-1b2f-457a-f269-cad05e624bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48239\n",
            "5139\n",
            "7\n",
            "2132        ar\n",
            "2132        ar\n",
            "2132        ar\n",
            "1614        ar\n",
            "2132        ar\n",
            "1614        ar\n",
            "3331        ar\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "5179       ckb\n",
            "54229       ja\n",
            "55492       ja\n",
            "47270       ja\n",
            "47270       ja\n",
            "55132       ja\n",
            "55132       ja\n",
            "51877       ja\n",
            "51685       ja\n",
            "47754       ja\n",
            "58534       ka\n",
            "58534       ka\n",
            "58534       ka\n",
            "58294       ka\n",
            "58294       ka\n",
            "58534       ka\n",
            "58294       ka\n",
            "58294       ka\n",
            "60600       ko\n",
            "60779       ko\n",
            "60449       ko\n",
            "60449       ko\n",
            "60600       ko\n",
            "60600       ko\n",
            "60449       ko\n",
            "60449       ko\n",
            "60449       ko\n",
            "60449       ko\n",
            "60600       ko\n",
            "60779       ko\n",
            "60779       ko\n",
            "60779       ko\n",
            "60600       ko\n",
            "60779       ko\n",
            "60779       ko\n",
            "60449       ko\n",
            "60600       ko\n",
            "7335        en\n",
            "13550       hi\n",
            "16171       hu\n",
            "23175       ko\n",
            "30212       ne\n",
            "40134       sd\n",
            "47047    zh-CN\n",
            "Name: lang, dtype: object\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we check the language distribution of our data \n",
        "print(df1[\"lang\"].value_counts())"
      ],
      "metadata": {
        "id": "fkQ_GtwuC83C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c53012a-aed8-4f2e-e56e-14932378088f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en       21156\n",
            "ja       11719\n",
            "es        7345\n",
            "id        3622\n",
            "pt        3199\n",
            "ar        3182\n",
            "ru        1841\n",
            "fr        1448\n",
            "hl        1142\n",
            "th        1037\n",
            "tr         947\n",
            "ka         846\n",
            "it         831\n",
            "si         827\n",
            "scb        812\n",
            "kn         797\n",
            "my         787\n",
            "el         777\n",
            "he         770\n",
            "bn         767\n",
            "nds        766\n",
            "ps         759\n",
            "km         752\n",
            "te         751\n",
            "ml         743\n",
            "pa         741\n",
            "gu         731\n",
            "ta         700\n",
            "hy         696\n",
            "am         678\n",
            "ko         666\n",
            "de         656\n",
            "hi         649\n",
            "lo         625\n",
            "nl         605\n",
            "ne         587\n",
            "sd         556\n",
            "zh-TW      538\n",
            "mr         525\n",
            "ckb        517\n",
            "zh-CN      461\n",
            "pl         441\n",
            "tl         441\n",
            "vi         429\n",
            "fa         419\n",
            "lv         290\n",
            "uk         214\n",
            "ca         209\n",
            "ro         185\n",
            "cs         172\n",
            "hu         165\n",
            "fi         165\n",
            "Name: lang, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u86aL9M0WA3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yt2qbRKOwz-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GR5qqqwoVAc"
      },
      "outputs": [],
      "source": [
        "# over and undersampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-test-split"
      ],
      "metadata": {
        "id": "Zj5h7QbnZC0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P9DYEq3y6pR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28974082-e8c6-4175-c315-b2a44622419e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999, 0.05199999999999999]\n"
          ]
        }
      ],
      "source": [
        "# Train-Validation-split\n",
        "\n",
        "\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(onehot[\"text\"], label_train, \n",
        "                                                                    random_state=12, \n",
        "                                                                    test_size=0.1, \n",
        "                                                                    stratify=label_train)\n",
        "\n",
        "test_text, test_labels = (onehot_t[\"text\"], label_test)\n",
        "\n",
        "\n",
        "\n",
        "relative_freq = []\n",
        "for c in train_labels.columns:\n",
        "  relative_freq.append(1/(1000*(train_labels[c].sum()/len(train_labels))))\n",
        "\n",
        "print(relative_freq)\n",
        "\n",
        "\n",
        "\n",
        "# testing whether the sampling is truly random \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#print(test_labels.iloc[:, : 10].head(10))"
      ],
      "metadata": {
        "id": "LH_j--aSitI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b387854-f3c8-438d-ed1a-6084b8d17f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ALL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALTCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALTCLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'ASTConfig', 'ASTFeatureExtractor', 'ASTForAudioClassification', 'ASTModel', 'ASTPreTrainedModel', 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'Adafactor', 'AdamW', 'AdamWeightDecay', 'AdaptiveEmbedding', 'AddedToken', 'AlbertConfig', 'AlbertForMaskedLM', 'AlbertForMultipleChoice', 'AlbertForPreTraining', 'AlbertForQuestionAnswering', 'AlbertForSequenceClassification', 'AlbertForTokenClassification', 'AlbertModel', 'AlbertPreTrainedModel', 'AlbertTokenizer', 'AlbertTokenizerFast', 'AltCLIPConfig', 'AltCLIPModel', 'AltCLIPPreTrainedModel', 'AltCLIPProcessor', 'AltCLIPTextConfig', 'AltCLIPTextModel', 'AltCLIPVisionConfig', 'AltCLIPVisionModel', 'AudioClassificationPipeline', 'AutoBackbone', 'AutoConfig', 'AutoFeatureExtractor', 'AutoImageProcessor', 'AutoModel', 'AutoModelForAudioClassification', 'AutoModelForAudioFrameClassification', 'AutoModelForAudioXVector', 'AutoModelForCTC', 'AutoModelForCausalLM', 'AutoModelForDepthEstimation', 'AutoModelForDocumentQuestionAnswering', 'AutoModelForImageClassification', 'AutoModelForImageSegmentation', 'AutoModelForInstanceSegmentation', 'AutoModelForMaskedImageModeling', 'AutoModelForMaskedLM', 'AutoModelForMultipleChoice', 'AutoModelForNextSentencePrediction', 'AutoModelForObjectDetection', 'AutoModelForPreTraining', 'AutoModelForQuestionAnswering', 'AutoModelForSemanticSegmentation', 'AutoModelForSeq2SeqLM', 'AutoModelForSequenceClassification', 'AutoModelForSpeechSeq2Seq', 'AutoModelForTableQuestionAnswering', 'AutoModelForTokenClassification', 'AutoModelForUniversalSegmentation', 'AutoModelForVideoClassification', 'AutoModelForVision2Seq', 'AutoModelForVisualQuestionAnswering', 'AutoModelForZeroShotObjectDetection', 'AutoModelWithLMHead', 'AutoProcessor', 'AutoTokenizer', 'AutomaticSpeechRecognitionPipeline', 'BART_PRETRAINED_MODEL_ARCHIVE_LIST', 'BEIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIGBIRD_PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIGBIRD_PEGASUS_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIG_BIRD_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIG_BIRD_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIOGPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIOGPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLENDERBOT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLENDERBOT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLENDERBOT_SMALL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLENDERBOT_SMALL_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLOOM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLOOM_PRETRAINED_MODEL_ARCHIVE_LIST', 'BartConfig', 'BartForCausalLM', 'BartForConditionalGeneration', 'BartForQuestionAnswering', 'BartForSequenceClassification', 'BartModel', 'BartPretrainedModel', 'BartTokenizer', 'BartTokenizerFast', 'BarthezTokenizer', 'BarthezTokenizerFast', 'BartphoTokenizer', 'BasicTokenizer', 'BatchEncoding', 'BatchFeature', 'BeamScorer', 'BeamSearchScorer', 'BeitConfig', 'BeitFeatureExtractor', 'BeitForImageClassification', 'BeitForMaskedImageModeling', 'BeitForSemanticSegmentation', 'BeitImageProcessor', 'BeitModel', 'BeitPreTrainedModel', 'BertConfig', 'BertForMaskedLM', 'BertForMultipleChoice', 'BertForNextSentencePrediction', 'BertForPreTraining', 'BertForQuestionAnswering', 'BertForSequenceClassification', 'BertForTokenClassification', 'BertGenerationConfig', 'BertGenerationDecoder', 'BertGenerationEncoder', 'BertGenerationPreTrainedModel', 'BertGenerationTokenizer', 'BertJapaneseTokenizer', 'BertLMHeadModel', 'BertLayer', 'BertModel', 'BertPreTrainedModel', 'BertTokenizer', 'BertTokenizerFast', 'BertweetTokenizer', 'BigBirdConfig', 'BigBirdForCausalLM', 'BigBirdForMaskedLM', 'BigBirdForMultipleChoice', 'BigBirdForPreTraining', 'BigBirdForQuestionAnswering', 'BigBirdForSequenceClassification', 'BigBirdForTokenClassification', 'BigBirdLayer', 'BigBirdModel', 'BigBirdPegasusConfig', 'BigBirdPegasusForCausalLM', 'BigBirdPegasusForConditionalGeneration', 'BigBirdPegasusForQuestionAnswering', 'BigBirdPegasusForSequenceClassification', 'BigBirdPegasusModel', 'BigBirdPegasusPreTrainedModel', 'BigBirdPreTrainedModel', 'BigBirdTokenizer', 'BigBirdTokenizerFast', 'BioGptConfig', 'BioGptForCausalLM', 'BioGptModel', 'BioGptPreTrainedModel', 'BioGptTokenizer', 'BitBackbone', 'BitConfig', 'BitForImageClassification', 'BitImageProcessor', 'BitModel', 'BitPreTrainedModel', 'BlenderbotConfig', 'BlenderbotForCausalLM', 'BlenderbotForConditionalGeneration', 'BlenderbotModel', 'BlenderbotPreTrainedModel', 'BlenderbotSmallConfig', 'BlenderbotSmallForCausalLM', 'BlenderbotSmallForConditionalGeneration', 'BlenderbotSmallModel', 'BlenderbotSmallPreTrainedModel', 'BlenderbotSmallTokenizer', 'BlenderbotSmallTokenizerFast', 'BlenderbotTokenizer', 'BlenderbotTokenizerFast', 'BlipConfig', 'BlipForConditionalGeneration', 'BlipForImageTextRetrieval', 'BlipForQuestionAnswering', 'BlipImageProcessor', 'BlipModel', 'BlipPreTrainedModel', 'BlipProcessor', 'BlipTextConfig', 'BlipTextModel', 'BlipVisionConfig', 'BlipVisionModel', 'BloomConfig', 'BloomForCausalLM', 'BloomForQuestionAnswering', 'BloomForSequenceClassification', 'BloomForTokenClassification', 'BloomModel', 'BloomPreTrainedModel', 'BloomTokenizerFast', 'ByT5Tokenizer', 'CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CANINE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CANINE_PRETRAINED_MODEL_ARCHIVE_LIST', 'CHINESE_CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CHINESE_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLIPConfig', 'CLIPFeatureExtractor', 'CLIPImageProcessor', 'CLIPModel', 'CLIPPreTrainedModel', 'CLIPProcessor', 'CLIPSEG_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CLIPSEG_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLIPSegConfig', 'CLIPSegForImageSegmentation', 'CLIPSegModel', 'CLIPSegPreTrainedModel', 'CLIPSegProcessor', 'CLIPSegTextConfig', 'CLIPSegTextModel', 'CLIPSegVisionConfig', 'CLIPSegVisionModel', 'CLIPTextConfig', 'CLIPTextModel', 'CLIPTextModelWithProjection', 'CLIPTokenizer', 'CLIPTokenizerFast', 'CLIPVisionConfig', 'CLIPVisionModel', 'CLIPVisionModelWithProjection', 'CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CODEGEN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONDITIONAL_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONDITIONAL_DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONFIG_MAPPING', 'CONFIG_NAME', 'CONVBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONVNEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVNEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CTRLConfig', 'CTRLForSequenceClassification', 'CTRLLMHeadModel', 'CTRLModel', 'CTRLPreTrainedModel', 'CTRLTokenizer', 'CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CTRL_PRETRAINED_MODEL_ARCHIVE_LIST', 'CVT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CVT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CamembertConfig', 'CamembertForCausalLM', 'CamembertForMaskedLM', 'CamembertForMultipleChoice', 'CamembertForQuestionAnswering', 'CamembertForSequenceClassification', 'CamembertForTokenClassification', 'CamembertModel', 'CamembertPreTrainedModel', 'CamembertTokenizer', 'CamembertTokenizerFast', 'CanineConfig', 'CanineForMultipleChoice', 'CanineForQuestionAnswering', 'CanineForSequenceClassification', 'CanineForTokenClassification', 'CanineLayer', 'CanineModel', 'CaninePreTrainedModel', 'CanineTokenizer', 'CharSpan', 'CharacterTokenizer', 'ChineseCLIPConfig', 'ChineseCLIPFeatureExtractor', 'ChineseCLIPImageProcessor', 'ChineseCLIPModel', 'ChineseCLIPPreTrainedModel', 'ChineseCLIPProcessor', 'ChineseCLIPTextConfig', 'ChineseCLIPTextModel', 'ChineseCLIPVisionConfig', 'ChineseCLIPVisionModel', 'CodeGenConfig', 'CodeGenForCausalLM', 'CodeGenModel', 'CodeGenPreTrainedModel', 'CodeGenTokenizer', 'CodeGenTokenizerFast', 'ConditionalDetrConfig', 'ConditionalDetrFeatureExtractor', 'ConditionalDetrForObjectDetection', 'ConditionalDetrForSegmentation', 'ConditionalDetrImageProcessor', 'ConditionalDetrModel', 'ConditionalDetrPreTrainedModel', 'ConstrainedBeamSearchScorer', 'Constraint', 'ConstraintListState', 'Conv1D', 'ConvBertConfig', 'ConvBertForMaskedLM', 'ConvBertForMultipleChoice', 'ConvBertForQuestionAnswering', 'ConvBertForSequenceClassification', 'ConvBertForTokenClassification', 'ConvBertLayer', 'ConvBertModel', 'ConvBertPreTrainedModel', 'ConvBertTokenizer', 'ConvBertTokenizerFast', 'ConvNextBackbone', 'ConvNextConfig', 'ConvNextFeatureExtractor', 'ConvNextForImageClassification', 'ConvNextImageProcessor', 'ConvNextModel', 'ConvNextPreTrainedModel', 'Conversation', 'ConversationalPipeline', 'CpmTokenizer', 'CpmTokenizerFast', 'CsvPipelineDataFormat', 'CvtConfig', 'CvtForImageClassification', 'CvtModel', 'CvtPreTrainedModel', 'DATA2VEC_AUDIO_PRETRAINED_MODEL_ARCHIVE_LIST', 'DATA2VEC_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DATA2VEC_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DATA2VEC_VISION_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DATA2VEC_VISION_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEBERTA_V2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'DECISION_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DECISION_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEFORMABLE_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEFORMABLE_DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'DINAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DINAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DONUT_SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DONUT_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPRConfig', 'DPRContextEncoder', 'DPRContextEncoderTokenizer', 'DPRContextEncoderTokenizerFast', 'DPRPreTrainedModel', 'DPRPretrainedContextEncoder', 'DPRPretrainedQuestionEncoder', 'DPRPretrainedReader', 'DPRQuestionEncoder', 'DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast', 'DPRReader', 'DPRReaderOutput', 'DPRReaderTokenizer', 'DPRReaderTokenizerFast', 'DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPTConfig', 'DPTFeatureExtractor', 'DPTForDepthEstimation', 'DPTForSemanticSegmentation', 'DPTImageProcessor', 'DPTModel', 'DPTPreTrainedModel', 'DPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'Data2VecAudioConfig', 'Data2VecAudioForAudioFrameClassification', 'Data2VecAudioForCTC', 'Data2VecAudioForSequenceClassification', 'Data2VecAudioForXVector', 'Data2VecAudioModel', 'Data2VecAudioPreTrainedModel', 'Data2VecTextConfig', 'Data2VecTextForCausalLM', 'Data2VecTextForMaskedLM', 'Data2VecTextForMultipleChoice', 'Data2VecTextForQuestionAnswering', 'Data2VecTextForSequenceClassification', 'Data2VecTextForTokenClassification', 'Data2VecTextModel', 'Data2VecTextPreTrainedModel', 'Data2VecVisionConfig', 'Data2VecVisionForImageClassification', 'Data2VecVisionForSemanticSegmentation', 'Data2VecVisionModel', 'Data2VecVisionPreTrainedModel', 'DataCollator', 'DataCollatorForLanguageModeling', 'DataCollatorForPermutationLanguageModeling', 'DataCollatorForSOP', 'DataCollatorForSeq2Seq', 'DataCollatorForTokenClassification', 'DataCollatorForWholeWordMask', 'DataCollatorWithPadding', 'DataProcessor', 'DebertaConfig', 'DebertaForMaskedLM', 'DebertaForQuestionAnswering', 'DebertaForSequenceClassification', 'DebertaForTokenClassification', 'DebertaModel', 'DebertaPreTrainedModel', 'DebertaTokenizer', 'DebertaTokenizerFast', 'DebertaV2Config', 'DebertaV2ForMaskedLM', 'DebertaV2ForMultipleChoice', 'DebertaV2ForQuestionAnswering', 'DebertaV2ForSequenceClassification', 'DebertaV2ForTokenClassification', 'DebertaV2Model', 'DebertaV2PreTrainedModel', 'DebertaV2Tokenizer', 'DebertaV2TokenizerFast', 'DecisionTransformerConfig', 'DecisionTransformerGPT2Model', 'DecisionTransformerGPT2PreTrainedModel', 'DecisionTransformerModel', 'DecisionTransformerPreTrainedModel', 'DefaultDataCollator', 'DefaultFlowCallback', 'DeformableDetrConfig', 'DeformableDetrFeatureExtractor', 'DeformableDetrForObjectDetection', 'DeformableDetrImageProcessor', 'DeformableDetrModel', 'DeformableDetrPreTrainedModel', 'DeiTConfig', 'DeiTFeatureExtractor', 'DeiTForImageClassification', 'DeiTForImageClassificationWithTeacher', 'DeiTForMaskedImageModeling', 'DeiTImageProcessor', 'DeiTModel', 'DeiTPreTrainedModel', 'DepthEstimationPipeline', 'DetrConfig', 'DetrFeatureExtractor', 'DetrForObjectDetection', 'DetrForSegmentation', 'DetrImageProcessor', 'DetrModel', 'DetrPreTrainedModel', 'DinatBackbone', 'DinatConfig', 'DinatForImageClassification', 'DinatModel', 'DinatPreTrainedModel', 'DisjunctiveConstraint', 'DistilBertConfig', 'DistilBertForMaskedLM', 'DistilBertForMultipleChoice', 'DistilBertForQuestionAnswering', 'DistilBertForSequenceClassification', 'DistilBertForTokenClassification', 'DistilBertModel', 'DistilBertPreTrainedModel', 'DistilBertTokenizer', 'DistilBertTokenizerFast', 'DocumentQuestionAnsweringPipeline', 'DonutFeatureExtractor', 'DonutImageProcessor', 'DonutProcessor', 'DonutSwinConfig', 'DonutSwinModel', 'DonutSwinPreTrainedModel', 'DummyObject', 'EFFICIENTFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'EFFICIENTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'ERNIE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ERNIE_PRETRAINED_MODEL_ARCHIVE_LIST', 'ESM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ESM_PRETRAINED_MODEL_ARCHIVE_LIST', 'EarlyStoppingCallback', 'EfficientFormerConfig', 'EfficientFormerForImageClassification', 'EfficientFormerForImageClassificationWithTeacher', 'EfficientFormerImageProcessor', 'EfficientFormerModel', 'EfficientFormerPreTrainedModel', 'ElectraConfig', 'ElectraForCausalLM', 'ElectraForMaskedLM', 'ElectraForMultipleChoice', 'ElectraForPreTraining', 'ElectraForQuestionAnswering', 'ElectraForSequenceClassification', 'ElectraForTokenClassification', 'ElectraModel', 'ElectraPreTrainedModel', 'ElectraTokenizer', 'ElectraTokenizerFast', 'EncoderDecoderConfig', 'EncoderDecoderModel', 'ErnieConfig', 'ErnieForCausalLM', 'ErnieForMaskedLM', 'ErnieForMultipleChoice', 'ErnieForNextSentencePrediction', 'ErnieForPreTraining', 'ErnieForQuestionAnswering', 'ErnieForSequenceClassification', 'ErnieForTokenClassification', 'ErnieModel', 'ErniePreTrainedModel', 'EsmConfig', 'EsmFoldPreTrainedModel', 'EsmForMaskedLM', 'EsmForProteinFolding', 'EsmForSequenceClassification', 'EsmForTokenClassification', 'EsmModel', 'EsmPreTrainedModel', 'EsmTokenizer', 'EvalPrediction', 'FEATURE_EXTRACTOR_MAPPING', 'FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'FLAVA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FLAVA_PRETRAINED_MODEL_ARCHIVE_LIST', 'FLAX_MODEL_FOR_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_MASKED_LM_MAPPING', 'FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'FLAX_MODEL_FOR_PRETRAINING_MAPPING', 'FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING', 'FLAX_MODEL_MAPPING', 'FLAX_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'FNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'FNetConfig', 'FNetForMaskedLM', 'FNetForMultipleChoice', 'FNetForNextSentencePrediction', 'FNetForPreTraining', 'FNetForQuestionAnswering', 'FNetForSequenceClassification', 'FNetForTokenClassification', 'FNetLayer', 'FNetModel', 'FNetPreTrainedModel', 'FNetTokenizer', 'FNetTokenizerFast', 'FSMTConfig', 'FSMTForConditionalGeneration', 'FSMTModel', 'FSMTTokenizer', 'FSMT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FUNNEL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST', 'FeatureExtractionMixin', 'FeatureExtractionPipeline', 'FillMaskPipeline', 'FlaubertConfig', 'FlaubertForMultipleChoice', 'FlaubertForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FlaubertForSequenceClassification', 'FlaubertForTokenClassification', 'FlaubertModel', 'FlaubertPreTrainedModel', 'FlaubertTokenizer', 'FlaubertWithLMHeadModel', 'FlavaConfig', 'FlavaFeatureExtractor', 'FlavaForPreTraining', 'FlavaImageCodebook', 'FlavaImageCodebookConfig', 'FlavaImageConfig', 'FlavaImageModel', 'FlavaImageProcessor', 'FlavaModel', 'FlavaMultimodalConfig', 'FlavaMultimodalModel', 'FlavaPreTrainedModel', 'FlavaProcessor', 'FlavaTextConfig', 'FlavaTextModel', 'FlaxAlbertForMaskedLM', 'FlaxAlbertForMultipleChoice', 'FlaxAlbertForPreTraining', 'FlaxAlbertForQuestionAnswering', 'FlaxAlbertForSequenceClassification', 'FlaxAlbertForTokenClassification', 'FlaxAlbertModel', 'FlaxAlbertPreTrainedModel', 'FlaxAutoModel', 'FlaxAutoModelForCausalLM', 'FlaxAutoModelForImageClassification', 'FlaxAutoModelForMaskedLM', 'FlaxAutoModelForMultipleChoice', 'FlaxAutoModelForNextSentencePrediction', 'FlaxAutoModelForPreTraining', 'FlaxAutoModelForQuestionAnswering', 'FlaxAutoModelForSeq2SeqLM', 'FlaxAutoModelForSequenceClassification', 'FlaxAutoModelForTokenClassification', 'FlaxAutoModelForVision2Seq', 'FlaxBartDecoderPreTrainedModel', 'FlaxBartForCausalLM', 'FlaxBartForConditionalGeneration', 'FlaxBartForQuestionAnswering', 'FlaxBartForSequenceClassification', 'FlaxBartModel', 'FlaxBartPreTrainedModel', 'FlaxBeitForImageClassification', 'FlaxBeitForMaskedImageModeling', 'FlaxBeitModel', 'FlaxBeitPreTrainedModel', 'FlaxBertForCausalLM', 'FlaxBertForMaskedLM', 'FlaxBertForMultipleChoice', 'FlaxBertForNextSentencePrediction', 'FlaxBertForPreTraining', 'FlaxBertForQuestionAnswering', 'FlaxBertForSequenceClassification', 'FlaxBertForTokenClassification', 'FlaxBertModel', 'FlaxBertPreTrainedModel', 'FlaxBigBirdForCausalLM', 'FlaxBigBirdForMaskedLM', 'FlaxBigBirdForMultipleChoice', 'FlaxBigBirdForPreTraining', 'FlaxBigBirdForQuestionAnswering', 'FlaxBigBirdForSequenceClassification', 'FlaxBigBirdForTokenClassification', 'FlaxBigBirdModel', 'FlaxBigBirdPreTrainedModel', 'FlaxBlenderbotForConditionalGeneration', 'FlaxBlenderbotModel', 'FlaxBlenderbotPreTrainedModel', 'FlaxBlenderbotSmallForConditionalGeneration', 'FlaxBlenderbotSmallModel', 'FlaxBlenderbotSmallPreTrainedModel', 'FlaxCLIPModel', 'FlaxCLIPPreTrainedModel', 'FlaxCLIPTextModel', 'FlaxCLIPTextPreTrainedModel', 'FlaxCLIPVisionModel', 'FlaxCLIPVisionPreTrainedModel', 'FlaxDistilBertForMaskedLM', 'FlaxDistilBertForMultipleChoice', 'FlaxDistilBertForQuestionAnswering', 'FlaxDistilBertForSequenceClassification', 'FlaxDistilBertForTokenClassification', 'FlaxDistilBertModel', 'FlaxDistilBertPreTrainedModel', 'FlaxElectraForCausalLM', 'FlaxElectraForMaskedLM', 'FlaxElectraForMultipleChoice', 'FlaxElectraForPreTraining', 'FlaxElectraForQuestionAnswering', 'FlaxElectraForSequenceClassification', 'FlaxElectraForTokenClassification', 'FlaxElectraModel', 'FlaxElectraPreTrainedModel', 'FlaxEncoderDecoderModel', 'FlaxForcedBOSTokenLogitsProcessor', 'FlaxForcedEOSTokenLogitsProcessor', 'FlaxGPT2LMHeadModel', 'FlaxGPT2Model', 'FlaxGPT2PreTrainedModel', 'FlaxGPTJForCausalLM', 'FlaxGPTJModel', 'FlaxGPTJPreTrainedModel', 'FlaxGPTNeoForCausalLM', 'FlaxGPTNeoModel', 'FlaxGPTNeoPreTrainedModel', 'FlaxGenerationMixin', 'FlaxLogitsProcessor', 'FlaxLogitsProcessorList', 'FlaxLogitsWarper', 'FlaxLongT5ForConditionalGeneration', 'FlaxLongT5Model', 'FlaxLongT5PreTrainedModel', 'FlaxMBartForConditionalGeneration', 'FlaxMBartForQuestionAnswering', 'FlaxMBartForSequenceClassification', 'FlaxMBartModel', 'FlaxMBartPreTrainedModel', 'FlaxMT5EncoderModel', 'FlaxMT5ForConditionalGeneration', 'FlaxMT5Model', 'FlaxMarianMTModel', 'FlaxMarianModel', 'FlaxMarianPreTrainedModel', 'FlaxMinLengthLogitsProcessor', 'FlaxOPTForCausalLM', 'FlaxOPTModel', 'FlaxOPTPreTrainedModel', 'FlaxPegasusForConditionalGeneration', 'FlaxPegasusModel', 'FlaxPegasusPreTrainedModel', 'FlaxPreTrainedModel', 'FlaxRoFormerForMaskedLM', 'FlaxRoFormerForMultipleChoice', 'FlaxRoFormerForQuestionAnswering', 'FlaxRoFormerForSequenceClassification', 'FlaxRoFormerForTokenClassification', 'FlaxRoFormerModel', 'FlaxRoFormerPreTrainedModel', 'FlaxRobertaForCausalLM', 'FlaxRobertaForMaskedLM', 'FlaxRobertaForMultipleChoice', 'FlaxRobertaForQuestionAnswering', 'FlaxRobertaForSequenceClassification', 'FlaxRobertaForTokenClassification', 'FlaxRobertaModel', 'FlaxRobertaPreLayerNormForCausalLM', 'FlaxRobertaPreLayerNormForMaskedLM', 'FlaxRobertaPreLayerNormForMultipleChoice', 'FlaxRobertaPreLayerNormForQuestionAnswering', 'FlaxRobertaPreLayerNormForSequenceClassification', 'FlaxRobertaPreLayerNormForTokenClassification', 'FlaxRobertaPreLayerNormModel', 'FlaxRobertaPreLayerNormPreTrainedModel', 'FlaxRobertaPreTrainedModel', 'FlaxSpeechEncoderDecoderModel', 'FlaxT5EncoderModel', 'FlaxT5ForConditionalGeneration', 'FlaxT5Model', 'FlaxT5PreTrainedModel', 'FlaxTemperatureLogitsWarper', 'FlaxTopKLogitsWarper', 'FlaxTopPLogitsWarper', 'FlaxViTForImageClassification', 'FlaxViTModel', 'FlaxViTPreTrainedModel', 'FlaxVisionEncoderDecoderModel', 'FlaxVisionTextDualEncoderModel', 'FlaxWav2Vec2ForCTC', 'FlaxWav2Vec2ForPreTraining', 'FlaxWav2Vec2Model', 'FlaxWav2Vec2PreTrainedModel', 'FlaxXGLMForCausalLM', 'FlaxXGLMModel', 'FlaxXGLMPreTrainedModel', 'FlaxXLMRobertaForCausalLM', 'FlaxXLMRobertaForMaskedLM', 'FlaxXLMRobertaForMultipleChoice', 'FlaxXLMRobertaForQuestionAnswering', 'FlaxXLMRobertaForSequenceClassification', 'FlaxXLMRobertaForTokenClassification', 'FlaxXLMRobertaModel', 'FlaxXLMRobertaPreTrainedModel', 'ForcedBOSTokenLogitsProcessor', 'ForcedEOSTokenLogitsProcessor', 'FunnelBaseModel', 'FunnelConfig', 'FunnelForMaskedLM', 'FunnelForMultipleChoice', 'FunnelForPreTraining', 'FunnelForQuestionAnswering', 'FunnelForSequenceClassification', 'FunnelForTokenClassification', 'FunnelModel', 'FunnelPreTrainedModel', 'FunnelTokenizer', 'FunnelTokenizerFast', 'GIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'GLPNConfig', 'GLPNFeatureExtractor', 'GLPNForDepthEstimation', 'GLPNImageProcessor', 'GLPNModel', 'GLPNPreTrainedModel', 'GLPN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GLPN_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT2Config', 'GPT2DoubleHeadsModel', 'GPT2ForSequenceClassification', 'GPT2ForTokenClassification', 'GPT2LMHeadModel', 'GPT2Model', 'GPT2PreTrainedModel', 'GPT2Tokenizer', 'GPT2TokenizerFast', 'GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT2_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTJConfig', 'GPTJForCausalLM', 'GPTJForQuestionAnswering', 'GPTJForSequenceClassification', 'GPTJModel', 'GPTJPreTrainedModel', 'GPTJ_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPTJ_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTNeoConfig', 'GPTNeoForCausalLM', 'GPTNeoForSequenceClassification', 'GPTNeoModel', 'GPTNeoPreTrainedModel', 'GPTNeoXConfig', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseConfig', 'GPTNeoXJapaneseForCausalLM', 'GPTNeoXJapaneseLayer', 'GPTNeoXJapaneseModel', 'GPTNeoXJapanesePreTrainedModel', 'GPTNeoXJapaneseTokenizer', 'GPTNeoXLayer', 'GPTNeoXModel', 'GPTNeoXPreTrainedModel', 'GPTNeoXTokenizerFast', 'GPTSw3Tokenizer', 'GPT_NEOX_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEOX_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEOX_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEO_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEO_PRETRAINED_MODEL_ARCHIVE_LIST', 'GRAPHORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GRAPHORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'GenerationConfig', 'GenerationMixin', 'GitConfig', 'GitForCausalLM', 'GitModel', 'GitPreTrainedModel', 'GitProcessor', 'GitVisionConfig', 'GitVisionModel', 'GlueDataTrainingArguments', 'GlueDataset', 'GradientAccumulator', 'GraphormerConfig', 'GraphormerForGraphClassification', 'GraphormerModel', 'GraphormerPreTrainedModel', 'GroupViTConfig', 'GroupViTModel', 'GroupViTPreTrainedModel', 'GroupViTTextConfig', 'GroupViTTextModel', 'GroupViTVisionConfig', 'GroupViTVisionModel', 'HUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'HammingDiversityLogitsProcessor', 'HerbertTokenizer', 'HerbertTokenizerFast', 'HfArgumentParser', 'HubertConfig', 'HubertForCTC', 'HubertForSequenceClassification', 'HubertModel', 'HubertPreTrainedModel', 'IBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'IBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'IBertConfig', 'IBertForMaskedLM', 'IBertForMultipleChoice', 'IBertForQuestionAnswering', 'IBertForSequenceClassification', 'IBertForTokenClassification', 'IBertModel', 'IBertPreTrainedModel', 'IMAGEGPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'IMAGEGPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'IMAGE_PROCESSOR_MAPPING', 'ImageClassificationPipeline', 'ImageFeatureExtractionMixin', 'ImageGPTConfig', 'ImageGPTFeatureExtractor', 'ImageGPTForCausalImageModeling', 'ImageGPTForImageClassification', 'ImageGPTImageProcessor', 'ImageGPTModel', 'ImageGPTPreTrainedModel', 'ImageProcessingMixin', 'ImageSegmentationPipeline', 'ImageToTextPipeline', 'InfNanRemoveLogitsProcessor', 'InputExample', 'InputFeatures', 'IntervalStrategy', 'JUKEBOX_PRETRAINED_CONFIG_ARCHIVE_MAP', 'JUKEBOX_PRETRAINED_MODEL_ARCHIVE_LIST', 'JsonPipelineDataFormat', 'JukeboxConfig', 'JukeboxModel', 'JukeboxPreTrainedModel', 'JukeboxPrior', 'JukeboxPriorConfig', 'JukeboxTokenizer', 'JukeboxVQVAE', 'JukeboxVQVAEConfig', 'KerasMetricCallback', 'LAYOUTLMV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLMV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'LAYOUTLMV3_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLMV3_PRETRAINED_MODEL_ARCHIVE_LIST', 'LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'LEDConfig', 'LEDForConditionalGeneration', 'LEDForQuestionAnswering', 'LEDForSequenceClassification', 'LEDModel', 'LEDPreTrainedModel', 'LEDTokenizer', 'LEDTokenizerFast', 'LED_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LED_PRETRAINED_MODEL_ARCHIVE_LIST', 'LEVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'LILT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LILT_PRETRAINED_MODEL_ARCHIVE_LIST', 'LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'LONGT5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGT5_PRETRAINED_MODEL_ARCHIVE_LIST', 'LUKE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LUKE_PRETRAINED_MODEL_ARCHIVE_LIST', 'LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LayoutLMConfig', 'LayoutLMForMaskedLM', 'LayoutLMForQuestionAnswering', 'LayoutLMForSequenceClassification', 'LayoutLMForTokenClassification', 'LayoutLMModel', 'LayoutLMPreTrainedModel', 'LayoutLMTokenizer', 'LayoutLMTokenizerFast', 'LayoutLMv2Config', 'LayoutLMv2FeatureExtractor', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv2ImageProcessor', 'LayoutLMv2Model', 'LayoutLMv2PreTrainedModel', 'LayoutLMv2Processor', 'LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast', 'LayoutLMv3Config', 'LayoutLMv3FeatureExtractor', 'LayoutLMv3ForQuestionAnswering', 'LayoutLMv3ForSequenceClassification', 'LayoutLMv3ForTokenClassification', 'LayoutLMv3ImageProcessor', 'LayoutLMv3Model', 'LayoutLMv3PreTrainedModel', 'LayoutLMv3Processor', 'LayoutLMv3Tokenizer', 'LayoutLMv3TokenizerFast', 'LayoutXLMProcessor', 'LayoutXLMTokenizer', 'LayoutXLMTokenizerFast', 'LevitConfig', 'LevitFeatureExtractor', 'LevitForImageClassification', 'LevitForImageClassificationWithTeacher', 'LevitImageProcessor', 'LevitModel', 'LevitPreTrainedModel', 'LiltConfig', 'LiltForQuestionAnswering', 'LiltForSequenceClassification', 'LiltForTokenClassification', 'LiltModel', 'LiltPreTrainedModel', 'LineByLineTextDataset', 'LineByLineWithRefDataset', 'LineByLineWithSOPTextDataset', 'LogitsProcessor', 'LogitsProcessorList', 'LogitsWarper', 'LongT5Config', 'LongT5EncoderModel', 'LongT5ForConditionalGeneration', 'LongT5Model', 'LongT5PreTrainedModel', 'LongformerConfig', 'LongformerForMaskedLM', 'LongformerForMultipleChoice', 'LongformerForQuestionAnswering', 'LongformerForSequenceClassification', 'LongformerForTokenClassification', 'LongformerModel', 'LongformerPreTrainedModel', 'LongformerSelfAttention', 'LongformerTokenizer', 'LongformerTokenizerFast', 'LukeConfig', 'LukeForEntityClassification', 'LukeForEntityPairClassification', 'LukeForEntitySpanClassification', 'LukeForMaskedLM', 'LukeForMultipleChoice', 'LukeForQuestionAnswering', 'LukeForSequenceClassification', 'LukeForTokenClassification', 'LukeModel', 'LukePreTrainedModel', 'LukeTokenizer', 'LxmertConfig', 'LxmertEncoder', 'LxmertForPreTraining', 'LxmertForQuestionAnswering', 'LxmertModel', 'LxmertPreTrainedModel', 'LxmertTokenizer', 'LxmertTokenizerFast', 'LxmertVisualFeatureEncoder', 'LxmertXLayer', 'M2M100Config', 'M2M100ForConditionalGeneration', 'M2M100Model', 'M2M100PreTrainedModel', 'M2M100Tokenizer', 'M2M_100_PRETRAINED_CONFIG_ARCHIVE_MAP', 'M2M_100_PRETRAINED_MODEL_ARCHIVE_LIST', 'MARKUPLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MARKUPLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'MASK2FORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MASK2FORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'MASKFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MASKFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'MBart50Tokenizer', 'MBart50TokenizerFast', 'MBartConfig', 'MBartForCausalLM', 'MBartForConditionalGeneration', 'MBartForQuestionAnswering', 'MBartForSequenceClassification', 'MBartModel', 'MBartPreTrainedModel', 'MBartTokenizer', 'MBartTokenizerFast', 'MCTCTConfig', 'MCTCTFeatureExtractor', 'MCTCTForCTC', 'MCTCTModel', 'MCTCTPreTrainedModel', 'MCTCTProcessor', 'MCTCT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MCTCT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MEGATRON_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MEGATRON_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MLukeTokenizer', 'MMBTConfig', 'MMBTForClassification', 'MMBTModel', 'MOBILEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILENET_V1_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILENET_V1_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILENET_V2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILENET_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILEVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MODEL_CARD_NAME', 'MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_XVECTOR_MAPPING', 'MODEL_FOR_BACKBONE_MAPPING', 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING', 'MODEL_FOR_CAUSAL_LM_MAPPING', 'MODEL_FOR_CTC_MAPPING', 'MODEL_FOR_DEPTH_ESTIMATION_MAPPING', 'MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_IMAGE_SEGMENTATION_MAPPING', 'MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING', 'MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'MODEL_FOR_MASKED_LM_MAPPING', 'MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'MODEL_FOR_OBJECT_DETECTION_MAPPING', 'MODEL_FOR_PRETRAINING_MAPPING', 'MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING', 'MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING', 'MODEL_FOR_VISION_2_SEQ_MAPPING', 'MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING', 'MODEL_MAPPING', 'MODEL_NAMES_MAPPING', 'MODEL_WITH_LM_HEAD_MAPPING', 'MPNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MPNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'MPNetConfig', 'MPNetForMaskedLM', 'MPNetForMultipleChoice', 'MPNetForQuestionAnswering', 'MPNetForSequenceClassification', 'MPNetForTokenClassification', 'MPNetLayer', 'MPNetModel', 'MPNetPreTrainedModel', 'MPNetTokenizer', 'MPNetTokenizerFast', 'MT5Config', 'MT5EncoderModel', 'MT5ForConditionalGeneration', 'MT5Model', 'MT5PreTrainedModel', 'MT5Tokenizer', 'MT5TokenizerFast', 'MVP_PRETRAINED_MODEL_ARCHIVE_LIST', 'MarianConfig', 'MarianForCausalLM', 'MarianMTModel', 'MarianModel', 'MarianTokenizer', 'MarkupLMConfig', 'MarkupLMFeatureExtractor', 'MarkupLMForQuestionAnswering', 'MarkupLMForSequenceClassification', 'MarkupLMForTokenClassification', 'MarkupLMModel', 'MarkupLMPreTrainedModel', 'MarkupLMProcessor', 'MarkupLMTokenizer', 'MarkupLMTokenizerFast', 'Mask2FormerConfig', 'Mask2FormerForUniversalSegmentation', 'Mask2FormerImageProcessor', 'Mask2FormerModel', 'Mask2FormerPreTrainedModel', 'MaskFormerConfig', 'MaskFormerFeatureExtractor', 'MaskFormerForInstanceSegmentation', 'MaskFormerImageProcessor', 'MaskFormerModel', 'MaskFormerPreTrainedModel', 'MaskFormerSwinBackbone', 'MaskFormerSwinConfig', 'MaxLengthCriteria', 'MaxTimeCriteria', 'MecabTokenizer', 'MegatronBertConfig', 'MegatronBertForCausalLM', 'MegatronBertForMaskedLM', 'MegatronBertForMultipleChoice', 'MegatronBertForNextSentencePrediction', 'MegatronBertForPreTraining', 'MegatronBertForQuestionAnswering', 'MegatronBertForSequenceClassification', 'MegatronBertForTokenClassification', 'MegatronBertModel', 'MegatronBertPreTrainedModel', 'MinLengthLogitsProcessor', 'MinNewTokensLengthLogitsProcessor', 'MobileBertConfig', 'MobileBertForMaskedLM', 'MobileBertForMultipleChoice', 'MobileBertForNextSentencePrediction', 'MobileBertForPreTraining', 'MobileBertForQuestionAnswering', 'MobileBertForSequenceClassification', 'MobileBertForTokenClassification', 'MobileBertLayer', 'MobileBertModel', 'MobileBertPreTrainedModel', 'MobileBertTokenizer', 'MobileBertTokenizerFast', 'MobileNetV1Config', 'MobileNetV1FeatureExtractor', 'MobileNetV1ForImageClassification', 'MobileNetV1ImageProcessor', 'MobileNetV1Model', 'MobileNetV1PreTrainedModel', 'MobileNetV2Config', 'MobileNetV2FeatureExtractor', 'MobileNetV2ForImageClassification', 'MobileNetV2ForSemanticSegmentation', 'MobileNetV2ImageProcessor', 'MobileNetV2Model', 'MobileNetV2PreTrainedModel', 'MobileViTConfig', 'MobileViTFeatureExtractor', 'MobileViTForImageClassification', 'MobileViTForSemanticSegmentation', 'MobileViTImageProcessor', 'MobileViTModel', 'MobileViTPreTrainedModel', 'ModalEmbeddings', 'ModelCard', 'MvpConfig', 'MvpForCausalLM', 'MvpForConditionalGeneration', 'MvpForQuestionAnswering', 'MvpForSequenceClassification', 'MvpModel', 'MvpPreTrainedModel', 'MvpTokenizer', 'MvpTokenizerFast', 'NAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'NEZHA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NEZHA_PRETRAINED_MODEL_ARCHIVE_LIST', 'NYSTROMFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NYSTROMFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'NatBackbone', 'NatConfig', 'NatForImageClassification', 'NatModel', 'NatPreTrainedModel', 'NerPipeline', 'NezhaConfig', 'NezhaForMaskedLM', 'NezhaForMultipleChoice', 'NezhaForNextSentencePrediction', 'NezhaForPreTraining', 'NezhaForQuestionAnswering', 'NezhaForSequenceClassification', 'NezhaForTokenClassification', 'NezhaModel', 'NezhaPreTrainedModel', 'NllbTokenizer', 'NllbTokenizerFast', 'NoBadWordsLogitsProcessor', 'NoRepeatNGramLogitsProcessor', 'NystromformerConfig', 'NystromformerForMaskedLM', 'NystromformerForMultipleChoice', 'NystromformerForQuestionAnswering', 'NystromformerForSequenceClassification', 'NystromformerForTokenClassification', 'NystromformerLayer', 'NystromformerModel', 'NystromformerPreTrainedModel', 'ONEFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ONEFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'OPTConfig', 'OPTForCausalLM', 'OPTForQuestionAnswering', 'OPTForSequenceClassification', 'OPTModel', 'OPTPreTrainedModel', 'OPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'OWLVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OWLVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ObjectDetectionPipeline', 'OneFormerConfig', 'OneFormerForUniversalSegmentation', 'OneFormerImageProcessor', 'OneFormerModel', 'OneFormerPreTrainedModel', 'OneFormerProcessor', 'OpenAIGPTConfig', 'OpenAIGPTDoubleHeadsModel', 'OpenAIGPTForSequenceClassification', 'OpenAIGPTLMHeadModel', 'OpenAIGPTModel', 'OpenAIGPTPreTrainedModel', 'OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast', 'OwlViTConfig', 'OwlViTFeatureExtractor', 'OwlViTForObjectDetection', 'OwlViTImageProcessor', 'OwlViTModel', 'OwlViTPreTrainedModel', 'OwlViTProcessor', 'OwlViTTextConfig', 'OwlViTTextModel', 'OwlViTVisionConfig', 'OwlViTVisionModel', 'PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PEGASUS_X_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PEGASUS_X_PRETRAINED_MODEL_ARCHIVE_LIST', 'PERCEIVER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PERCEIVER_PRETRAINED_MODEL_ARCHIVE_LIST', 'PLBART_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PLBART_PRETRAINED_MODEL_ARCHIVE_LIST', 'PLBartConfig', 'PLBartForCausalLM', 'PLBartForConditionalGeneration', 'PLBartForSequenceClassification', 'PLBartModel', 'PLBartPreTrainedModel', 'PLBartTokenizer', 'POOLFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'POOLFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'PROCESSOR_MAPPING', 'PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'PYTORCH_PRETRAINED_BERT_CACHE', 'PYTORCH_TRANSFORMERS_CACHE', 'PegasusConfig', 'PegasusForCausalLM', 'PegasusForConditionalGeneration', 'PegasusModel', 'PegasusPreTrainedModel', 'PegasusTokenizer', 'PegasusTokenizerFast', 'PegasusXConfig', 'PegasusXForConditionalGeneration', 'PegasusXModel', 'PegasusXPreTrainedModel', 'PerceiverConfig', 'PerceiverFeatureExtractor', 'PerceiverForImageClassificationConvProcessing', 'PerceiverForImageClassificationFourier', 'PerceiverForImageClassificationLearned', 'PerceiverForMaskedLM', 'PerceiverForMultimodalAutoencoding', 'PerceiverForOpticalFlow', 'PerceiverForSequenceClassification', 'PerceiverImageProcessor', 'PerceiverLayer', 'PerceiverModel', 'PerceiverPreTrainedModel', 'PerceiverTokenizer', 'PhobertTokenizer', 'PhrasalConstraint', 'PipedPipelineDataFormat', 'Pipeline', 'PipelineDataFormat', 'PoolFormerConfig', 'PoolFormerFeatureExtractor', 'PoolFormerForImageClassification', 'PoolFormerImageProcessor', 'PoolFormerModel', 'PoolFormerPreTrainedModel', 'PreTrainedModel', 'PreTrainedTokenizer', 'PreTrainedTokenizerBase', 'PreTrainedTokenizerFast', 'PrefixConstrainedLogitsProcessor', 'PretrainedBartModel', 'PretrainedConfig', 'PretrainedFSMTModel', 'PrinterCallback', 'ProcessorMixin', 'ProgressCallback', 'ProphetNetConfig', 'ProphetNetDecoder', 'ProphetNetEncoder', 'ProphetNetForCausalLM', 'ProphetNetForConditionalGeneration', 'ProphetNetModel', 'ProphetNetPreTrainedModel', 'ProphetNetTokenizer', 'PushToHubCallback', 'PyTorchBenchmark', 'PyTorchBenchmarkArguments', 'QDQBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'QDQBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'QDQBertConfig', 'QDQBertForMaskedLM', 'QDQBertForMultipleChoice', 'QDQBertForNextSentencePrediction', 'QDQBertForQuestionAnswering', 'QDQBertForSequenceClassification', 'QDQBertForTokenClassification', 'QDQBertLMHeadModel', 'QDQBertLayer', 'QDQBertModel', 'QDQBertPreTrainedModel', 'QuestionAnsweringPipeline', 'REALM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REALM_PRETRAINED_MODEL_ARCHIVE_LIST', 'REFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'REGNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REGNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'REMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'RESNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RESNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROBERTA_PRELAYERNORM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROBERTA_PRELAYERNORM_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROC_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROC_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'RagConfig', 'RagModel', 'RagPreTrainedModel', 'RagRetriever', 'RagSequenceForGeneration', 'RagTokenForGeneration', 'RagTokenizer', 'RealmConfig', 'RealmEmbedder', 'RealmForOpenQA', 'RealmKnowledgeAugEncoder', 'RealmPreTrainedModel', 'RealmReader', 'RealmRetriever', 'RealmScorer', 'RealmTokenizer', 'RealmTokenizerFast', 'ReformerAttention', 'ReformerConfig', 'ReformerForMaskedLM', 'ReformerForQuestionAnswering', 'ReformerForSequenceClassification', 'ReformerLayer', 'ReformerModel', 'ReformerModelWithLMHead', 'ReformerPreTrainedModel', 'ReformerTokenizer', 'ReformerTokenizerFast', 'RegNetConfig', 'RegNetForImageClassification', 'RegNetModel', 'RegNetPreTrainedModel', 'RemBertConfig', 'RemBertForCausalLM', 'RemBertForMaskedLM', 'RemBertForMultipleChoice', 'RemBertForQuestionAnswering', 'RemBertForSequenceClassification', 'RemBertForTokenClassification', 'RemBertLayer', 'RemBertModel', 'RemBertPreTrainedModel', 'RemBertTokenizer', 'RemBertTokenizerFast', 'RepetitionPenaltyLogitsProcessor', 'ResNetBackbone', 'ResNetConfig', 'ResNetForImageClassification', 'ResNetModel', 'ResNetPreTrainedModel', 'RetriBertConfig', 'RetriBertModel', 'RetriBertPreTrainedModel', 'RetriBertTokenizer', 'RetriBertTokenizerFast', 'RoCBertConfig', 'RoCBertForCausalLM', 'RoCBertForMaskedLM', 'RoCBertForMultipleChoice', 'RoCBertForPreTraining', 'RoCBertForQuestionAnswering', 'RoCBertForSequenceClassification', 'RoCBertForTokenClassification', 'RoCBertLayer', 'RoCBertModel', 'RoCBertPreTrainedModel', 'RoCBertTokenizer', 'RoFormerConfig', 'RoFormerForCausalLM', 'RoFormerForMaskedLM', 'RoFormerForMultipleChoice', 'RoFormerForQuestionAnswering', 'RoFormerForSequenceClassification', 'RoFormerForTokenClassification', 'RoFormerLayer', 'RoFormerModel', 'RoFormerPreTrainedModel', 'RoFormerTokenizer', 'RoFormerTokenizerFast', 'RobertaConfig', 'RobertaForCausalLM', 'RobertaForMaskedLM', 'RobertaForMultipleChoice', 'RobertaForQuestionAnswering', 'RobertaForSequenceClassification', 'RobertaForTokenClassification', 'RobertaModel', 'RobertaPreLayerNormConfig', 'RobertaPreLayerNormForCausalLM', 'RobertaPreLayerNormForMaskedLM', 'RobertaPreLayerNormForMultipleChoice', 'RobertaPreLayerNormForQuestionAnswering', 'RobertaPreLayerNormForSequenceClassification', 'RobertaPreLayerNormForTokenClassification', 'RobertaPreLayerNormModel', 'RobertaPreLayerNormPreTrainedModel', 'RobertaPreTrainedModel', 'RobertaTokenizer', 'RobertaTokenizerFast', 'SEGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEWConfig', 'SEWDConfig', 'SEWDForCTC', 'SEWDForSequenceClassification', 'SEWDModel', 'SEWDPreTrainedModel', 'SEWForCTC', 'SEWForSequenceClassification', 'SEWModel', 'SEWPreTrainedModel', 'SEW_D_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEW_D_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEW_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEW_PRETRAINED_MODEL_ARCHIVE_LIST', 'SLOW_TO_FAST_CONVERTERS', 'SPEECH_TO_TEXT_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECH_TO_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECH_TO_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'SPIECE_UNDERLINE', 'SPLINTER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPLINTER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SQUEEZEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SQUEEZEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIN2SR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIN2SR_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWINV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWINV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWITCH_TRANSFORMERS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWITCH_TRANSFORMERS_PRETRAINED_MODEL_ARCHIVE_LIST', 'SchedulerType', 'SegformerConfig', 'SegformerDecodeHead', 'SegformerFeatureExtractor', 'SegformerForImageClassification', 'SegformerForSemanticSegmentation', 'SegformerImageProcessor', 'SegformerLayer', 'SegformerModel', 'SegformerPreTrainedModel', 'Seq2SeqTrainer', 'Seq2SeqTrainingArguments', 'SequenceFeatureExtractor', 'SingleSentenceClassificationProcessor', 'SpecialTokensMixin', 'Speech2Text2Config', 'Speech2Text2ForCausalLM', 'Speech2Text2PreTrainedModel', 'Speech2Text2Processor', 'Speech2Text2Tokenizer', 'Speech2TextConfig', 'Speech2TextFeatureExtractor', 'Speech2TextForConditionalGeneration', 'Speech2TextModel', 'Speech2TextPreTrainedModel', 'Speech2TextProcessor', 'Speech2TextTokenizer', 'SpeechEncoderDecoderConfig', 'SpeechEncoderDecoderModel', 'SplinterConfig', 'SplinterForPreTraining', 'SplinterForQuestionAnswering', 'SplinterLayer', 'SplinterModel', 'SplinterPreTrainedModel', 'SplinterTokenizer', 'SplinterTokenizerFast', 'SquadDataTrainingArguments', 'SquadDataset', 'SquadExample', 'SquadFeatures', 'SquadV1Processor', 'SquadV2Processor', 'SqueezeBertConfig', 'SqueezeBertForMaskedLM', 'SqueezeBertForMultipleChoice', 'SqueezeBertForQuestionAnswering', 'SqueezeBertForSequenceClassification', 'SqueezeBertForTokenClassification', 'SqueezeBertModel', 'SqueezeBertModule', 'SqueezeBertPreTrainedModel', 'SqueezeBertTokenizer', 'SqueezeBertTokenizerFast', 'StoppingCriteria', 'StoppingCriteriaList', 'SummarizationPipeline', 'Swin2SRConfig', 'Swin2SRForImageSuperResolution', 'Swin2SRImageProcessor', 'Swin2SRModel', 'Swin2SRPreTrainedModel', 'SwinBackbone', 'SwinConfig', 'SwinForImageClassification', 'SwinForMaskedImageModeling', 'SwinModel', 'SwinPreTrainedModel', 'Swinv2Config', 'Swinv2ForImageClassification', 'Swinv2ForMaskedImageModeling', 'Swinv2Model', 'Swinv2PreTrainedModel', 'SwitchTransformersConfig', 'SwitchTransformersEncoderModel', 'SwitchTransformersForConditionalGeneration', 'SwitchTransformersModel', 'SwitchTransformersPreTrainedModel', 'SwitchTransformersSparseMLP', 'SwitchTransformersTop1Router', 'T5Config', 'T5EncoderModel', 'T5ForConditionalGeneration', 'T5Model', 'T5PreTrainedModel', 'T5Tokenizer', 'T5TokenizerFast', 'T5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'T5_PRETRAINED_MODEL_ARCHIVE_LIST', 'TABLE_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TABLE_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TAPAS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TAPAS_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF2_WEIGHTS_NAME', 'TFAdaptiveEmbedding', 'TFAlbertForMaskedLM', 'TFAlbertForMultipleChoice', 'TFAlbertForPreTraining', 'TFAlbertForQuestionAnswering', 'TFAlbertForSequenceClassification', 'TFAlbertForTokenClassification', 'TFAlbertMainLayer', 'TFAlbertModel', 'TFAlbertPreTrainedModel', 'TFAutoModel', 'TFAutoModelForCausalLM', 'TFAutoModelForDocumentQuestionAnswering', 'TFAutoModelForImageClassification', 'TFAutoModelForMaskedLM', 'TFAutoModelForMultipleChoice', 'TFAutoModelForNextSentencePrediction', 'TFAutoModelForPreTraining', 'TFAutoModelForQuestionAnswering', 'TFAutoModelForSemanticSegmentation', 'TFAutoModelForSeq2SeqLM', 'TFAutoModelForSequenceClassification', 'TFAutoModelForSpeechSeq2Seq', 'TFAutoModelForTableQuestionAnswering', 'TFAutoModelForTokenClassification', 'TFAutoModelForVision2Seq', 'TFAutoModelWithLMHead', 'TFBartForConditionalGeneration', 'TFBartForSequenceClassification', 'TFBartModel', 'TFBartPretrainedModel', 'TFBertEmbeddings', 'TFBertForMaskedLM', 'TFBertForMultipleChoice', 'TFBertForNextSentencePrediction', 'TFBertForPreTraining', 'TFBertForQuestionAnswering', 'TFBertForSequenceClassification', 'TFBertForTokenClassification', 'TFBertLMHeadModel', 'TFBertMainLayer', 'TFBertModel', 'TFBertPreTrainedModel', 'TFBertTokenizer', 'TFBlenderbotForConditionalGeneration', 'TFBlenderbotModel', 'TFBlenderbotPreTrainedModel', 'TFBlenderbotSmallForConditionalGeneration', 'TFBlenderbotSmallModel', 'TFBlenderbotSmallPreTrainedModel', 'TFCLIPModel', 'TFCLIPPreTrainedModel', 'TFCLIPTextModel', 'TFCLIPVisionModel', 'TFCTRLForSequenceClassification', 'TFCTRLLMHeadModel', 'TFCTRLModel', 'TFCTRLPreTrainedModel', 'TFCamembertForCausalLM', 'TFCamembertForMaskedLM', 'TFCamembertForMultipleChoice', 'TFCamembertForQuestionAnswering', 'TFCamembertForSequenceClassification', 'TFCamembertForTokenClassification', 'TFCamembertModel', 'TFCamembertPreTrainedModel', 'TFConvBertForMaskedLM', 'TFConvBertForMultipleChoice', 'TFConvBertForQuestionAnswering', 'TFConvBertForSequenceClassification', 'TFConvBertForTokenClassification', 'TFConvBertLayer', 'TFConvBertModel', 'TFConvBertPreTrainedModel', 'TFConvNextForImageClassification', 'TFConvNextModel', 'TFConvNextPreTrainedModel', 'TFCvtForImageClassification', 'TFCvtModel', 'TFCvtPreTrainedModel', 'TFDPRContextEncoder', 'TFDPRPretrainedContextEncoder', 'TFDPRPretrainedQuestionEncoder', 'TFDPRPretrainedReader', 'TFDPRQuestionEncoder', 'TFDPRReader', 'TFData2VecVisionForImageClassification', 'TFData2VecVisionForSemanticSegmentation', 'TFData2VecVisionModel', 'TFData2VecVisionPreTrainedModel', 'TFDebertaForMaskedLM', 'TFDebertaForQuestionAnswering', 'TFDebertaForSequenceClassification', 'TFDebertaForTokenClassification', 'TFDebertaModel', 'TFDebertaPreTrainedModel', 'TFDebertaV2ForMaskedLM', 'TFDebertaV2ForQuestionAnswering', 'TFDebertaV2ForSequenceClassification', 'TFDebertaV2ForTokenClassification', 'TFDebertaV2Model', 'TFDebertaV2PreTrainedModel', 'TFDeiTForImageClassification', 'TFDeiTForImageClassificationWithTeacher', 'TFDeiTForMaskedImageModeling', 'TFDeiTModel', 'TFDeiTPreTrainedModel', 'TFDistilBertForMaskedLM', 'TFDistilBertForMultipleChoice', 'TFDistilBertForQuestionAnswering', 'TFDistilBertForSequenceClassification', 'TFDistilBertForTokenClassification', 'TFDistilBertMainLayer', 'TFDistilBertModel', 'TFDistilBertPreTrainedModel', 'TFElectraForMaskedLM', 'TFElectraForMultipleChoice', 'TFElectraForPreTraining', 'TFElectraForQuestionAnswering', 'TFElectraForSequenceClassification', 'TFElectraForTokenClassification', 'TFElectraModel', 'TFElectraPreTrainedModel', 'TFEncoderDecoderModel', 'TFEsmForMaskedLM', 'TFEsmForSequenceClassification', 'TFEsmForTokenClassification', 'TFEsmModel', 'TFEsmPreTrainedModel', 'TFFlaubertForMultipleChoice', 'TFFlaubertForQuestionAnsweringSimple', 'TFFlaubertForSequenceClassification', 'TFFlaubertForTokenClassification', 'TFFlaubertModel', 'TFFlaubertPreTrainedModel', 'TFFlaubertWithLMHeadModel', 'TFForcedBOSTokenLogitsProcessor', 'TFForcedEOSTokenLogitsProcessor', 'TFFunnelBaseModel', 'TFFunnelForMaskedLM', 'TFFunnelForMultipleChoice', 'TFFunnelForPreTraining', 'TFFunnelForQuestionAnswering', 'TFFunnelForSequenceClassification', 'TFFunnelForTokenClassification', 'TFFunnelModel', 'TFFunnelPreTrainedModel', 'TFGPT2DoubleHeadsModel', 'TFGPT2ForSequenceClassification', 'TFGPT2LMHeadModel', 'TFGPT2MainLayer', 'TFGPT2Model', 'TFGPT2PreTrainedModel', 'TFGPT2Tokenizer', 'TFGPTJForCausalLM', 'TFGPTJForQuestionAnswering', 'TFGPTJForSequenceClassification', 'TFGPTJModel', 'TFGPTJPreTrainedModel', 'TFGenerationMixin', 'TFGroupViTModel', 'TFGroupViTPreTrainedModel', 'TFGroupViTTextModel', 'TFGroupViTVisionModel', 'TFHubertForCTC', 'TFHubertModel', 'TFHubertPreTrainedModel', 'TFLEDForConditionalGeneration', 'TFLEDModel', 'TFLEDPreTrainedModel', 'TFLayoutLMForMaskedLM', 'TFLayoutLMForQuestionAnswering', 'TFLayoutLMForSequenceClassification', 'TFLayoutLMForTokenClassification', 'TFLayoutLMMainLayer', 'TFLayoutLMModel', 'TFLayoutLMPreTrainedModel', 'TFLayoutLMv3ForQuestionAnswering', 'TFLayoutLMv3ForSequenceClassification', 'TFLayoutLMv3ForTokenClassification', 'TFLayoutLMv3Model', 'TFLayoutLMv3PreTrainedModel', 'TFLogitsProcessor', 'TFLogitsProcessorList', 'TFLogitsWarper', 'TFLongformerForMaskedLM', 'TFLongformerForMultipleChoice', 'TFLongformerForQuestionAnswering', 'TFLongformerForSequenceClassification', 'TFLongformerForTokenClassification', 'TFLongformerModel', 'TFLongformerPreTrainedModel', 'TFLongformerSelfAttention', 'TFLxmertForPreTraining', 'TFLxmertMainLayer', 'TFLxmertModel', 'TFLxmertPreTrainedModel', 'TFLxmertVisualFeatureEncoder', 'TFMBartForConditionalGeneration', 'TFMBartModel', 'TFMBartPreTrainedModel', 'TFMPNetForMaskedLM', 'TFMPNetForMultipleChoice', 'TFMPNetForQuestionAnswering', 'TFMPNetForSequenceClassification', 'TFMPNetForTokenClassification', 'TFMPNetMainLayer', 'TFMPNetModel', 'TFMPNetPreTrainedModel', 'TFMT5EncoderModel', 'TFMT5ForConditionalGeneration', 'TFMT5Model', 'TFMarianMTModel', 'TFMarianModel', 'TFMarianPreTrainedModel', 'TFMinLengthLogitsProcessor', 'TFMobileBertForMaskedLM', 'TFMobileBertForMultipleChoice', 'TFMobileBertForNextSentencePrediction', 'TFMobileBertForPreTraining', 'TFMobileBertForQuestionAnswering', 'TFMobileBertForSequenceClassification', 'TFMobileBertForTokenClassification', 'TFMobileBertMainLayer', 'TFMobileBertModel', 'TFMobileBertPreTrainedModel', 'TFMobileViTForImageClassification', 'TFMobileViTForSemanticSegmentation', 'TFMobileViTModel', 'TFMobileViTPreTrainedModel', 'TFNoBadWordsLogitsProcessor', 'TFNoRepeatNGramLogitsProcessor', 'TFOPTForCausalLM', 'TFOPTModel', 'TFOPTPreTrainedModel', 'TFOpenAIGPTDoubleHeadsModel', 'TFOpenAIGPTForSequenceClassification', 'TFOpenAIGPTLMHeadModel', 'TFOpenAIGPTMainLayer', 'TFOpenAIGPTModel', 'TFOpenAIGPTPreTrainedModel', 'TFPegasusForConditionalGeneration', 'TFPegasusModel', 'TFPegasusPreTrainedModel', 'TFPreTrainedModel', 'TFRagModel', 'TFRagPreTrainedModel', 'TFRagSequenceForGeneration', 'TFRagTokenForGeneration', 'TFRegNetForImageClassification', 'TFRegNetModel', 'TFRegNetPreTrainedModel', 'TFRemBertForCausalLM', 'TFRemBertForMaskedLM', 'TFRemBertForMultipleChoice', 'TFRemBertForQuestionAnswering', 'TFRemBertForSequenceClassification', 'TFRemBertForTokenClassification', 'TFRemBertLayer', 'TFRemBertModel', 'TFRemBertPreTrainedModel', 'TFRepetitionPenaltyLogitsProcessor', 'TFResNetForImageClassification', 'TFResNetModel', 'TFResNetPreTrainedModel', 'TFRoFormerForCausalLM', 'TFRoFormerForMaskedLM', 'TFRoFormerForMultipleChoice', 'TFRoFormerForQuestionAnswering', 'TFRoFormerForSequenceClassification', 'TFRoFormerForTokenClassification', 'TFRoFormerLayer', 'TFRoFormerModel', 'TFRoFormerPreTrainedModel', 'TFRobertaForCausalLM', 'TFRobertaForMaskedLM', 'TFRobertaForMultipleChoice', 'TFRobertaForQuestionAnswering', 'TFRobertaForSequenceClassification', 'TFRobertaForTokenClassification', 'TFRobertaMainLayer', 'TFRobertaModel', 'TFRobertaPreLayerNormForCausalLM', 'TFRobertaPreLayerNormForMaskedLM', 'TFRobertaPreLayerNormForMultipleChoice', 'TFRobertaPreLayerNormForQuestionAnswering', 'TFRobertaPreLayerNormForSequenceClassification', 'TFRobertaPreLayerNormForTokenClassification', 'TFRobertaPreLayerNormMainLayer', 'TFRobertaPreLayerNormModel', 'TFRobertaPreLayerNormPreTrainedModel', 'TFRobertaPreTrainedModel', 'TFSegformerDecodeHead', 'TFSegformerForImageClassification', 'TFSegformerForSemanticSegmentation', 'TFSegformerModel', 'TFSegformerPreTrainedModel', 'TFSequenceSummary', 'TFSharedEmbeddings', 'TFSpeech2TextForConditionalGeneration', 'TFSpeech2TextModel', 'TFSpeech2TextPreTrainedModel', 'TFSwinForImageClassification', 'TFSwinForMaskedImageModeling', 'TFSwinModel', 'TFSwinPreTrainedModel', 'TFT5EncoderModel', 'TFT5ForConditionalGeneration', 'TFT5Model', 'TFT5PreTrainedModel', 'TFTapasForMaskedLM', 'TFTapasForQuestionAnswering', 'TFTapasForSequenceClassification', 'TFTapasModel', 'TFTapasPreTrainedModel', 'TFTemperatureLogitsWarper', 'TFTopKLogitsWarper', 'TFTopPLogitsWarper', 'TFTrainer', 'TFTrainingArguments', 'TFTransfoXLForSequenceClassification', 'TFTransfoXLLMHeadModel', 'TFTransfoXLMainLayer', 'TFTransfoXLModel', 'TFTransfoXLPreTrainedModel', 'TFViTForImageClassification', 'TFViTMAEForPreTraining', 'TFViTMAEModel', 'TFViTMAEPreTrainedModel', 'TFViTModel', 'TFViTPreTrainedModel', 'TFVisionEncoderDecoderModel', 'TFWav2Vec2ForCTC', 'TFWav2Vec2Model', 'TFWav2Vec2PreTrainedModel', 'TFWhisperForConditionalGeneration', 'TFWhisperModel', 'TFWhisperPreTrainedModel', 'TFXGLMForCausalLM', 'TFXGLMModel', 'TFXGLMPreTrainedModel', 'TFXLMForMultipleChoice', 'TFXLMForQuestionAnsweringSimple', 'TFXLMForSequenceClassification', 'TFXLMForTokenClassification', 'TFXLMMainLayer', 'TFXLMModel', 'TFXLMPreTrainedModel', 'TFXLMRobertaForCausalLM', 'TFXLMRobertaForMaskedLM', 'TFXLMRobertaForMultipleChoice', 'TFXLMRobertaForQuestionAnswering', 'TFXLMRobertaForSequenceClassification', 'TFXLMRobertaForTokenClassification', 'TFXLMRobertaModel', 'TFXLMRobertaPreTrainedModel', 'TFXLMWithLMHeadModel', 'TFXLNetForMultipleChoice', 'TFXLNetForQuestionAnsweringSimple', 'TFXLNetForSequenceClassification', 'TFXLNetForTokenClassification', 'TFXLNetLMHeadModel', 'TFXLNetMainLayer', 'TFXLNetModel', 'TFXLNetPreTrainedModel', 'TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CTRL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CVT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_GPT2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LAYOUTLMV3_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LXMERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MOBILEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MODEL_FOR_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'TF_MODEL_FOR_MASKED_LM_MAPPING', 'TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'TF_MODEL_FOR_PRETRAINING_MAPPING', 'TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_VISION_2_SEQ_MAPPING', 'TF_MODEL_MAPPING', 'TF_MODEL_WITH_LM_HEAD_MAPPING', 'TF_MPNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_REGNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_REMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_RESNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROBERTA_PRELAYERNORM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SEGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SPEECH_TO_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_T5_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_TAPAS_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_WAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_WEIGHTS_NAME', 'TF_WHISPER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XGLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TIMESFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TIMESFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TIME_SERIES_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TIME_SERIES_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TOKENIZER_MAPPING', 'TRAJECTORY_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TRAJECTORY_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TRANSFORMERS_CACHE', 'TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TROCR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TROCR_PRETRAINED_MODEL_ARCHIVE_LIST', 'TableQuestionAnsweringPipeline', 'TableTransformerConfig', 'TableTransformerForObjectDetection', 'TableTransformerModel', 'TableTransformerPreTrainedModel', 'TapasConfig', 'TapasForMaskedLM', 'TapasForQuestionAnswering', 'TapasForSequenceClassification', 'TapasModel', 'TapasPreTrainedModel', 'TapasTokenizer', 'TapexTokenizer', 'TemperatureLogitsWarper', 'TensorFlowBenchmark', 'TensorFlowBenchmarkArguments', 'TensorType', 'Text2TextGenerationPipeline', 'TextClassificationPipeline', 'TextDataset', 'TextDatasetForNextSentencePrediction', 'TextGenerationPipeline', 'TimeSeriesTransformerConfig', 'TimeSeriesTransformerForPrediction', 'TimeSeriesTransformerModel', 'TimeSeriesTransformerPreTrainedModel', 'TimesformerConfig', 'TimesformerForVideoClassification', 'TimesformerModel', 'TimesformerPreTrainedModel', 'TokenClassificationPipeline', 'TokenSpan', 'TopKLogitsWarper', 'TopPLogitsWarper', 'TrOCRConfig', 'TrOCRForCausalLM', 'TrOCRPreTrainedModel', 'TrOCRProcessor', 'Trainer', 'TrainerCallback', 'TrainerControl', 'TrainerState', 'TrainingArguments', 'TrajectoryTransformerConfig', 'TrajectoryTransformerModel', 'TrajectoryTransformerPreTrainedModel', 'TransfoXLConfig', 'TransfoXLCorpus', 'TransfoXLForSequenceClassification', 'TransfoXLLMHeadModel', 'TransfoXLModel', 'TransfoXLPreTrainedModel', 'TransfoXLTokenizer', 'TranslationPipeline', 'TypicalLogitsWarper', 'UNISPEECH_PRETRAINED_CONFIG_ARCHIVE_MAP', 'UNISPEECH_PRETRAINED_MODEL_ARCHIVE_LIST', 'UNISPEECH_SAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'UNISPEECH_SAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'UniSpeechConfig', 'UniSpeechForCTC', 'UniSpeechForPreTraining', 'UniSpeechForSequenceClassification', 'UniSpeechModel', 'UniSpeechPreTrainedModel', 'UniSpeechSatConfig', 'UniSpeechSatForAudioFrameClassification', 'UniSpeechSatForCTC', 'UniSpeechSatForPreTraining', 'UniSpeechSatForSequenceClassification', 'UniSpeechSatForXVector', 'UniSpeechSatModel', 'UniSpeechSatPreTrainedModel', 'UperNetConfig', 'UperNetForSemanticSegmentation', 'UperNetPreTrainedModel', 'VAN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VAN_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIDEOMAE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIDEOMAE_PRETRAINED_MODEL_ARCHIVE_LIST', 'VILT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VILT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VISUAL_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VISUAL_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_HYBRID_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_HYBRID_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_MAE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_MAE_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_MSN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_MSN_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VanConfig', 'VanForImageClassification', 'VanModel', 'VanPreTrainedModel', 'ViTConfig', 'ViTFeatureExtractor', 'ViTForImageClassification', 'ViTForMaskedImageModeling', 'ViTHybridConfig', 'ViTHybridForImageClassification', 'ViTHybridImageProcessor', 'ViTHybridModel', 'ViTHybridPreTrainedModel', 'ViTImageProcessor', 'ViTMAEConfig', 'ViTMAEForPreTraining', 'ViTMAELayer', 'ViTMAEModel', 'ViTMAEPreTrainedModel', 'ViTMSNConfig', 'ViTMSNForImageClassification', 'ViTMSNModel', 'ViTMSNPreTrainedModel', 'ViTModel', 'ViTPreTrainedModel', 'VideoClassificationPipeline', 'VideoMAEConfig', 'VideoMAEFeatureExtractor', 'VideoMAEForPreTraining', 'VideoMAEForVideoClassification', 'VideoMAEImageProcessor', 'VideoMAEModel', 'VideoMAEPreTrainedModel', 'ViltConfig', 'ViltFeatureExtractor', 'ViltForImageAndTextRetrieval', 'ViltForImagesAndTextClassification', 'ViltForMaskedLM', 'ViltForQuestionAnswering', 'ViltForTokenClassification', 'ViltImageProcessor', 'ViltLayer', 'ViltModel', 'ViltPreTrainedModel', 'ViltProcessor', 'VisionEncoderDecoderConfig', 'VisionEncoderDecoderModel', 'VisionTextDualEncoderConfig', 'VisionTextDualEncoderModel', 'VisionTextDualEncoderProcessor', 'VisualBertConfig', 'VisualBertForMultipleChoice', 'VisualBertForPreTraining', 'VisualBertForQuestionAnswering', 'VisualBertForRegionToPhraseAlignment', 'VisualBertForVisualReasoning', 'VisualBertLayer', 'VisualBertModel', 'VisualBertPreTrainedModel', 'VisualQuestionAnsweringPipeline', 'WAV2VEC2_CONFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAV2VEC2_CONFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'WAVLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAVLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'WAV_2_VEC_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'WEIGHTS_NAME', 'WHISPER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WHISPER_PRETRAINED_MODEL_ARCHIVE_LIST', 'WarmUp', 'Wav2Vec2CTCTokenizer', 'Wav2Vec2Config', 'Wav2Vec2ConformerConfig', 'Wav2Vec2ConformerForAudioFrameClassification', 'Wav2Vec2ConformerForCTC', 'Wav2Vec2ConformerForPreTraining', 'Wav2Vec2ConformerForSequenceClassification', 'Wav2Vec2ConformerForXVector', 'Wav2Vec2ConformerModel', 'Wav2Vec2ConformerPreTrainedModel', 'Wav2Vec2FeatureExtractor', 'Wav2Vec2ForAudioFrameClassification', 'Wav2Vec2ForCTC', 'Wav2Vec2ForMaskedLM', 'Wav2Vec2ForPreTraining', 'Wav2Vec2ForSequenceClassification', 'Wav2Vec2ForXVector', 'Wav2Vec2Model', 'Wav2Vec2PhonemeCTCTokenizer', 'Wav2Vec2PreTrainedModel', 'Wav2Vec2Processor', 'Wav2Vec2ProcessorWithLM', 'Wav2Vec2Tokenizer', 'WavLMConfig', 'WavLMForAudioFrameClassification', 'WavLMForCTC', 'WavLMForSequenceClassification', 'WavLMForXVector', 'WavLMModel', 'WavLMPreTrainedModel', 'WhisperConfig', 'WhisperFeatureExtractor', 'WhisperForConditionalGeneration', 'WhisperModel', 'WhisperPreTrainedModel', 'WhisperProcessor', 'WhisperTokenizer', 'WordpieceTokenizer', 'XCLIPConfig', 'XCLIPModel', 'XCLIPPreTrainedModel', 'XCLIPProcessor', 'XCLIPTextConfig', 'XCLIPTextModel', 'XCLIPVisionConfig', 'XCLIPVisionModel', 'XCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XCLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'XGLMConfig', 'XGLMForCausalLM', 'XGLMModel', 'XGLMPreTrainedModel', 'XGLMTokenizer', 'XGLMTokenizerFast', 'XGLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XGLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLMConfig', 'XLMForMultipleChoice', 'XLMForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMForSequenceClassification', 'XLMForTokenClassification', 'XLMModel', 'XLMPreTrainedModel', 'XLMProphetNetConfig', 'XLMProphetNetDecoder', 'XLMProphetNetEncoder', 'XLMProphetNetForCausalLM', 'XLMProphetNetForConditionalGeneration', 'XLMProphetNetModel', 'XLMProphetNetPreTrainedModel', 'XLMProphetNetTokenizer', 'XLMRobertaConfig', 'XLMRobertaForCausalLM', 'XLMRobertaForMaskedLM', 'XLMRobertaForMultipleChoice', 'XLMRobertaForQuestionAnswering', 'XLMRobertaForSequenceClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaModel', 'XLMRobertaPreTrainedModel', 'XLMRobertaTokenizer', 'XLMRobertaTokenizerFast', 'XLMRobertaXLConfig', 'XLMRobertaXLForCausalLM', 'XLMRobertaXLForMaskedLM', 'XLMRobertaXLForMultipleChoice', 'XLMRobertaXLForQuestionAnswering', 'XLMRobertaXLForSequenceClassification', 'XLMRobertaXLForTokenClassification', 'XLMRobertaXLModel', 'XLMRobertaXLPreTrainedModel', 'XLMTokenizer', 'XLMWithLMHeadModel', 'XLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_ROBERTA_XL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_ROBERTA_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLNetConfig', 'XLNetForMultipleChoice', 'XLNetForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XLNetForSequenceClassification', 'XLNetForTokenClassification', 'XLNetLMHeadModel', 'XLNetModel', 'XLNetPreTrainedModel', 'XLNetTokenizer', 'XLNetTokenizerFast', 'YOLOS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'YOLOS_PRETRAINED_MODEL_ARCHIVE_LIST', 'YOSO_PRETRAINED_CONFIG_ARCHIVE_MAP', 'YOSO_PRETRAINED_MODEL_ARCHIVE_LIST', 'YolosConfig', 'YolosFeatureExtractor', 'YolosForObjectDetection', 'YolosImageProcessor', 'YolosModel', 'YolosPreTrainedModel', 'YosoConfig', 'YosoForMaskedLM', 'YosoForMultipleChoice', 'YosoForQuestionAnswering', 'YosoForSequenceClassification', 'YosoForTokenClassification', 'YosoLayer', 'YosoModel', 'YosoPreTrainedModel', 'ZeroShotClassificationPipeline', 'ZeroShotImageClassificationPipeline', 'ZeroShotObjectDetectionPipeline', '__all__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_class_to_module', '_import_structure', '_modules', '_name', '_objects', 'activations', 'activations_tf', 'add_end_docstrings', 'add_start_docstrings', 'apply_chunking_to_forward', 'benchmark', 'benchmark.benchmark', 'benchmark.benchmark_args', 'benchmark.benchmark_args_tf', 'benchmark.benchmark_tf', 'commands', 'configuration_utils', 'convert_graph_to_onnx', 'convert_slow_tokenizer', 'convert_slow_tokenizers_checkpoints_to_fast', 'convert_tf_hub_seq_to_seq_bert_to_pytorch', 'convert_tf_weight_name_to_pt_weight_name', 'create_optimizer', 'data', 'data.data_collator', 'data.datasets', 'data.metrics', 'data.processors', 'debug_utils', 'deepspeed', 'default_data_collator', 'dependency_versions_check', 'dependency_versions_table', 'dynamic_module_utils', 'enable_full_determinism', 'feature_extraction_sequence_utils', 'feature_extraction_utils', 'file_utils', 'generation', 'generation_tf_utils', 'generation_utils', 'get_constant_schedule', 'get_constant_schedule_with_warmup', 'get_cosine_schedule_with_warmup', 'get_cosine_with_hard_restarts_schedule_with_warmup', 'get_linear_schedule_with_warmup', 'get_polynomial_decay_schedule_with_warmup', 'get_scheduler', 'glue_compute_metrics', 'glue_convert_examples_to_features', 'glue_output_modes', 'glue_processors', 'glue_tasks_num_labels', 'hf_argparser', 'image_processing_utils', 'image_transforms', 'image_utils', 'integrations', 'is_apex_available', 'is_clearml_available', 'is_comet_available', 'is_datasets_available', 'is_decord_available', 'is_faiss_available', 'is_flax_available', 'is_keras_nlp_available', 'is_neptune_available', 'is_optuna_available', 'is_phonemizer_available', 'is_psutil_available', 'is_py3nvml_available', 'is_pyctcdecode_available', 'is_ray_available', 'is_ray_tune_available', 'is_safetensors_available', 'is_scipy_available', 'is_sentencepiece_available', 'is_sigopt_available', 'is_sklearn_available', 'is_speech_available', 'is_tensorboard_available', 'is_tensorflow_text_available', 'is_tf_available', 'is_timm_available', 'is_tokenizers_available', 'is_torch_available', 'is_torch_neuroncore_available', 'is_torch_tpu_available', 'is_vision_available', 'is_wandb_available', 'keras_callbacks', 'load_pytorch_checkpoint_in_tf2_model', 'load_pytorch_model_in_tf2_model', 'load_pytorch_weights_in_tf2_model', 'load_tf2_checkpoint_in_pytorch_model', 'load_tf2_model_in_pytorch_model', 'load_tf2_weights_in_pytorch_model', 'load_tf_weights_in_albert', 'load_tf_weights_in_bert', 'load_tf_weights_in_bert_generation', 'load_tf_weights_in_big_bird', 'load_tf_weights_in_canine', 'load_tf_weights_in_convbert', 'load_tf_weights_in_electra', 'load_tf_weights_in_funnel', 'load_tf_weights_in_gpt2', 'load_tf_weights_in_gpt_neo', 'load_tf_weights_in_imagegpt', 'load_tf_weights_in_mobilebert', 'load_tf_weights_in_mobilenet_v1', 'load_tf_weights_in_mobilenet_v2', 'load_tf_weights_in_openai_gpt', 'load_tf_weights_in_qdqbert', 'load_tf_weights_in_realm', 'load_tf_weights_in_rembert', 'load_tf_weights_in_roc_bert', 'load_tf_weights_in_roformer', 'load_tf_weights_in_t5', 'load_tf_weights_in_tapas', 'load_tf_weights_in_transfo_xl', 'load_tf_weights_in_xlnet', 'logging', 'modelcard', 'modeling_outputs', 'modeling_tf_outputs', 'modeling_tf_pytorch_utils', 'modeling_tf_utils', 'modeling_utils', 'models', 'models.albert', 'models.altclip', 'models.audio_spectrogram_transformer', 'models.auto', 'models.bart', 'models.barthez', 'models.bartpho', 'models.beit', 'models.bert', 'models.bert_generation', 'models.bert_japanese', 'models.bertweet', 'models.big_bird', 'models.bigbird_pegasus', 'models.biogpt', 'models.bit', 'models.blenderbot', 'models.blenderbot_small', 'models.blip', 'models.bloom', 'models.bort', 'models.byt5', 'models.camembert', 'models.canine', 'models.chinese_clip', 'models.clip', 'models.clipseg', 'models.codegen', 'models.conditional_detr', 'models.convbert', 'models.convnext', 'models.cpm', 'models.ctrl', 'models.cvt', 'models.data2vec', 'models.deberta', 'models.deberta_v2', 'models.decision_transformer', 'models.deformable_detr', 'models.deit', 'models.detr', 'models.dialogpt', 'models.dinat', 'models.distilbert', 'models.dit', 'models.donut', 'models.dpr', 'models.dpt', 'models.efficientformer', 'models.electra', 'models.encoder_decoder', 'models.ernie', 'models.esm', 'models.flaubert', 'models.flava', 'models.fnet', 'models.fsmt', 'models.funnel', 'models.git', 'models.glpn', 'models.gpt2', 'models.gpt_neo', 'models.gpt_neox', 'models.gpt_neox_japanese', 'models.gpt_sw3', 'models.gptj', 'models.graphormer', 'models.groupvit', 'models.herbert', 'models.hubert', 'models.ibert', 'models.imagegpt', 'models.jukebox', 'models.layoutlm', 'models.layoutlmv2', 'models.layoutlmv3', 'models.layoutxlm', 'models.led', 'models.levit', 'models.lilt', 'models.longformer', 'models.longt5', 'models.luke', 'models.lxmert', 'models.m2m_100', 'models.marian', 'models.markuplm', 'models.mask2former', 'models.maskformer', 'models.mbart', 'models.mbart50', 'models.mctct', 'models.megatron_bert', 'models.megatron_gpt2', 'models.mluke', 'models.mmbt', 'models.mobilebert', 'models.mobilenet_v1', 'models.mobilenet_v2', 'models.mobilevit', 'models.mpnet', 'models.mt5', 'models.mvp', 'models.nat', 'models.nezha', 'models.nllb', 'models.nystromformer', 'models.oneformer', 'models.openai', 'models.opt', 'models.owlvit', 'models.pegasus', 'models.pegasus_x', 'models.perceiver', 'models.phobert', 'models.plbart', 'models.poolformer', 'models.prophetnet', 'models.qdqbert', 'models.rag', 'models.realm', 'models.reformer', 'models.regnet', 'models.rembert', 'models.resnet', 'models.retribert', 'models.roberta', 'models.roberta_prelayernorm', 'models.roc_bert', 'models.roformer', 'models.segformer', 'models.sew', 'models.sew_d', 'models.speech_encoder_decoder', 'models.speech_to_text', 'models.speech_to_text_2', 'models.splinter', 'models.squeezebert', 'models.swin', 'models.swin2sr', 'models.swinv2', 'models.switch_transformers', 'models.t5', 'models.table_transformer', 'models.tapas', 'models.tapex', 'models.time_series_transformer', 'models.timesformer', 'models.trajectory_transformer', 'models.transfo_xl', 'models.trocr', 'models.unispeech', 'models.unispeech_sat', 'models.upernet', 'models.van', 'models.videomae', 'models.vilt', 'models.vision_encoder_decoder', 'models.vision_text_dual_encoder', 'models.visual_bert', 'models.vit', 'models.vit_hybrid', 'models.vit_mae', 'models.vit_msn', 'models.wav2vec2', 'models.wav2vec2_conformer', 'models.wav2vec2_phoneme', 'models.wav2vec2_with_lm', 'models.wavlm', 'models.whisper', 'models.x_clip', 'models.xglm', 'models.xlm', 'models.xlm_prophetnet', 'models.xlm_roberta', 'models.xlm_roberta_xl', 'models.xlnet', 'models.yolos', 'models.yoso', 'onnx', 'optimization', 'optimization_tf', 'pipeline', 'pipelines', 'processing_utils', 'prune_layer', 'pytorch_utils', 'requires_backends', 'sagemaker', 'set_seed', 'shape_list', 'squad_convert_examples_to_features', 'testing_utils', 'tf_top_k_top_p_filtering', 'tf_utils', 'tokenization_utils', 'tokenization_utils_base', 'tokenization_utils_fast', 'top_k_top_p_filtering', 'torch_distributed_zero_first', 'trainer', 'trainer_callback', 'trainer_pt_utils', 'trainer_seq2seq', 'trainer_tf', 'trainer_utils', 'training_args', 'training_args_seq2seq', 'training_args_tf', 'utils', 'utils.bitsandbytes', 'utils.dummy_flax_objects', 'utils.dummy_keras_nlp_objects', 'utils.dummy_sentencepiece_and_speech_objects', 'utils.dummy_sentencepiece_and_tokenizers_objects', 'utils.dummy_sentencepiece_objects', 'utils.dummy_tensorflow_text_objects', 'utils.dummy_timm_and_vision_objects', 'xnli_compute_metrics', 'xnli_output_modes', 'xnli_processors', 'xnli_tasks_num_labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert Fine-tuning"
      ],
      "metadata": {
        "id": "2ItKm0hDMujq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pretrained bert Model\n"
      ],
      "metadata": {
        "id": "HIkcURDhz2R0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6skrhdVzW0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250,
          "referenced_widgets": [
            "20ffdc30026144179acd3e0408bb47eb",
            "fd7212dec9dc48b6b33461975ac9a97f",
            "829eac4629414e29b03b2d87dc93b76c",
            "e5164128b64e4edd82bf9663bd98c6fd",
            "53eb7aa1d0aa4c6c826ab6672c2acf93",
            "1c6e1e6453334776815c405373187c26",
            "3ad39e8eb5fb4a4c8fa73f81b4e8f5c7",
            "5d12416775424b289f6e0f1cf326672c",
            "272f49bed543492e8fc4b3c0667952bf",
            "bc047b2e49d0499ab5bfbf13da8e1d06",
            "6fcd9c828830485ab0d45fcf2a7be1f2",
            "1f337c25e3bb43ec8c5ad523b7e09630",
            "86879b1a27224703bfb37094b88672e8",
            "cecbfedbfe4e4097a8b54466df1975e7",
            "d61b85ff749b4c30bb3216db74511a7c",
            "df9ec7c1246a47d592c183e6624cd5e0",
            "2c2216f3a19d464b97bcf536e9f4f02c",
            "25b1439962144c8984930c3ccc9cf6c9",
            "69eec1c2d08e43b1afe78ca0b5b8ddf2",
            "f760e8a1d63449b6b0914d2dcc52e989",
            "d39169c6967d4be5827e9d5b685d8152",
            "3a96212020b147af8125d7eb0f783e77",
            "5ac0c469fd1f4df4ba604a5bda0a8532",
            "bca60bcfd3f943f398985388870c2e4d",
            "f8c28e1640f54a8a988bb569e5d96b04",
            "b11c27960e1d442fa870f2203f29ad5f",
            "30bbf8ea27014993812a85738bf102fd",
            "f91a006f93914fe8bb6a572187f2b8c1",
            "503185f26ab44233a664a9afb05f0d69",
            "e2b408dbb46541bea2f4bf4d9215fd17",
            "7c552150fc6c42fcb7608efc250a979d",
            "fec15901d7a04a8cbc6d903010603302",
            "80ff18e5c638465d8b4dbb5c299dd92a",
            "35f8d88bf0f24682849751478c204b23",
            "e77fb2067ed449a3ab4e9cc9c6c969e0",
            "e54d43c5dda548449a7125bd3bbde461",
            "2900b6badd334b2c996ff148628e3e68",
            "b6d901ae11cd49f28f989ddf8ce12af5",
            "df54c816e09146168a68e8d8da094fa2",
            "735b956dcc6f4657b828f20a3cfe717f",
            "8145f43c723543b1a9c89755ae51ad13",
            "f3dd5c0337ec45bf897be6bdb2952437",
            "ec2be56b197745698e3e800fafcc857a",
            "5a27774d34a241ec8a3f7539667bce26",
            "b961f3045c234bb78a4f6140678fc68d",
            "63b95eb2a2894b98ad5e97b0fa415683",
            "2afbbdced5c94aa1a6fb6e80c33c9e74",
            "6a80d09600be4c43809b4edad2d54e9c",
            "6b150548ded6416b875b879be2a10150",
            "36286784162a482e8f4d12a1bb07836e",
            "d3cd0c381f5145a59b2f7c1200434a18",
            "6ca178193b5a4af1b83c1e4c4a838c66",
            "679173f824ab453291c8866f5e099a00",
            "8b518b02f5a340d6baab58001216e774",
            "29443d8a3e5c490da8d41379acb63a63"
          ]
        },
        "outputId": "3b71f440-eef2-43b3-b2f8-2e1d1a57a895"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20ffdc30026144179acd3e0408bb47eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/672M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f337c25e3bb43ec8c5ad523b7e09630"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ac0c469fd1f4df4ba604a5bda0a8532"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35f8d88bf0f24682849751478c204b23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b961f3045c234bb78a4f6140678fc68d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load pretrained BERT model and tokenizer \n",
        "from transformers import AutoTokenizer\n",
        "model_name ='bert-base-multilingual-uncased' # or 'roberta-base' or 'xlm-roberta-base' or 'cardiffnlp/twitter-xlm-roberta-base' \n",
        "bert = AutoModel.from_pretrained(model_name,output_hidden_states=True) # for bert model \n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased') #  DistilBertTokenizer bert-base-multilingual-uncased"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert.modules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JkW205lUR6G",
        "outputId": "0bfdd5d8-0dcf-4c2b-85ff-044211560330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.modules of XLMRobertaModel(\n",
            "  (embeddings): XLMRobertaEmbeddings(\n",
            "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "    (token_type_embeddings): Embedding(1, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): XLMRobertaEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): XLMRobertaLayer(\n",
            "        (attention): XLMRobertaAttention(\n",
            "          (self): XLMRobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): XLMRobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): XLMRobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): XLMRobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): XLMRobertaPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of word distribution in training data"
      ],
      "metadata": {
        "id": "H1G4xtl7dXBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDZx-O2xzcA_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "c87733b6-748e-4447-b04e-19b3ca64db93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of tweets')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# visualiation of the \n",
        "seq_len = [len(title.split()) for title in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 40)\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Number of tweets')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ry0ptLTW-Chy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c4678dce-78a1-4a64-a0d2-44061913149e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from torch.nn.utils.rnn import pad_sequence\\n\\ninput_text = [\"text sequence 1\", \"text sequence 2\"]\\n\\n# Tokenize the input text\\ntokenized_text = [tokenizer.tokenize(text) for text in input_text]\\n\\n# Convert the tokenized text to numerical representation\\ninput_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_text]\\n\\n# Pad the sequences to a specified max length\\npadded_input_ids = pad_sequence(torch.tensor(input_ids), batch_first=True, padding_value=tokenizer.pad_token_id)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfwINnZozkyd"
      },
      "outputs": [],
      "source": [
        "# maxlength of 25 seems reasonable according to the prior visualization\n",
        "MAX_LENGHT = 25\n",
        "# Tokenize and encode sequences in the train set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = MAX_LENGHT,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = MAX_LENGHT,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = MAX_LENGHT,\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE9PI9_H0Moi"
      },
      "outputs": [],
      "source": [
        "# Convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.values)\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.values)\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oft-16jR0M6h"
      },
      "outputs": [],
      "source": [
        "# Data Loader structure definition\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 64                                               \n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors\n",
        "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "                                                           \n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)            \n",
        "val_sampler = SequentialSampler(val_data)                    \n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "                                                            "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freeze Layers"
      ],
      "metadata": {
        "id": "VkHiWeqop3Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing the parameters \n",
        "# We also tried to train the last pooler layer but did not gain a significiant performance boost\n",
        "\"\"\"for name, param in bert.named_parameters():\n",
        "    if 'pooler' not in name:\n",
        "        param.requires_grad = False\"\"\"\n",
        "\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False \n",
        "      # false -> no gradient\"\"\""
      ],
      "metadata": {
        "id": "dH-wI1yhzQkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert.classifier = nn.Linear(768,52)\n",
        "#temp = nn.Linear(768,52)\n",
        "number_trainparam = sum(p.numel() for p in bert.parameters() if p.requires_grad) # check if we only train the layers we want to train\n",
        "print(number_trainparam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj67gfCZ_O5d",
        "outputId": "e9f5aa73-a405-41d6-b25f-1313e227b677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCdOnqJGXU8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYOt3-pAZ8f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSxW9ixOn0RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition Model Architecture"
      ],
      "metadata": {
        "id": "XAJPyGH4zaRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oC6f5jD0vm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13854a29-eb01-4bbd-860c-dc0a00d58810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):  \n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert   \n",
        "      self.dropout = nn.Dropout(0.1)           \n",
        "      self.relu =  nn.ReLU()                    \n",
        "      self.fc1 = nn.Linear(768,128)          \n",
        "      self.fc2 = nn.Linear(128,52)               # dense layer 2 (Output layer)\n",
        "           \n",
        "    def forward(self, sent_id, mask):           \n",
        "      \n",
        "      cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      # we take the mean over all the last hidden states                                         \n",
        "      x = self.fc1(torch.mean(cls_hs.last_hidden_state,1))\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)     # no softmax is required since the binary cross entropy loss already incorporates softmax                       \n",
        "                             \n",
        "      return x\n",
        "\n",
        "\n",
        "model = BERT_Arch(bert).to(device=device) # send model to device\n",
        "\n",
        "# Define the loss function\n",
        "# weight = torch.tensor(relative_freq)\n",
        "cross_entropy  = nn.CrossEntropyLoss().to(device=device)\n",
        "\n",
        "# Defining the optimizer\n",
        "\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(),lr = 3e-4)          # learning rate\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rn4Hg-fGnoCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition of training and eval\n"
      ],
      "metadata": {
        "id": "LGFpP494zgLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEG81NvL1Rt9"
      },
      "outputs": [],
      "source": [
        "# Defining training and evaluation functions\n",
        "def train():  \n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
        "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    batch = [r for r in batch]                                  \n",
        "    sent_id, mask, labels = batch \n",
        "    \n",
        "    sent_id = sent_id.to(device=device) # send all elements on the gpu \n",
        "    mask = mask.to(device=device) \n",
        "                                              \n",
        "    preds = model(sent_id, mask)\n",
        "    \n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "    # get model predictions for current batch\n",
        "    optimizer.zero_grad() \n",
        "    labels = labels.float().to(device=device)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "      \n",
        "                       \n",
        "    total_loss = total_loss + loss.item()                       # add up loss\n",
        "    loss.backward()                                             # calculate the gradients\n",
        "    optimizer.step()                                            # update parameters\n",
        "    preds=preds.detach().cpu().numpy()                          # model predictions stored on GPU-> CPU\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch  \n",
        "                                                               \n",
        "  return avg_loss                                 # returns the loss and predictions\n",
        "\n",
        "def evaluate():  \n",
        "  print(\"\\nEvaluation...\")  \n",
        "  model.eval()                                    # Deactivate dropout layers\n",
        "  total_loss, total_accuracy = 0, 0  \n",
        "  for step,batch in enumerate(val_dataloader):    # Iterate over batches  \n",
        "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.     \n",
        "                                                \n",
        "                                                \n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "                                                 \n",
        "    batch = [t for t in batch]                    \n",
        "    sent_id, mask, labels = batch\n",
        "    with torch.no_grad():   \n",
        "      sent_id = sent_id.to(device=device)\n",
        "      mask = sent_id.to(device=device)  \n",
        "      labels = labels.float().to(device=device)\n",
        "\n",
        "      preds = model(sent_id, mask)  \n",
        "              \n",
        "      loss = cross_entropy(preds,labels )                      \n",
        "      \"\"\"preds = model(sent_id, mask)                # Model predictions\n",
        "      loss = cross_entropy(preds,labels.long())    \"\"\"     \n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "  avg_loss = total_loss / len(val_dataloader)         # the validation loss of the epoch\n",
        "  return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDvfL_V6lMRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "IARpecoUzloO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSN0ZPiW1d-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db071d8-21f8-4fc5-a10a-6bb8da1ed63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 1.165\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.366\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.309\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.289\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.276\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.272\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.268\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.264\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.261\n",
            "Validation Loss: nan\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of  3,088.\n",
            "  Batch   100  of  3,088.\n",
            "  Batch   150  of  3,088.\n",
            "  Batch   200  of  3,088.\n",
            "  Batch   250  of  3,088.\n",
            "  Batch   300  of  3,088.\n",
            "  Batch   350  of  3,088.\n",
            "  Batch   400  of  3,088.\n",
            "  Batch   450  of  3,088.\n",
            "  Batch   500  of  3,088.\n",
            "  Batch   550  of  3,088.\n",
            "  Batch   600  of  3,088.\n",
            "  Batch   650  of  3,088.\n",
            "  Batch   700  of  3,088.\n",
            "  Batch   750  of  3,088.\n",
            "  Batch   800  of  3,088.\n",
            "  Batch   850  of  3,088.\n",
            "  Batch   900  of  3,088.\n",
            "  Batch   950  of  3,088.\n",
            "  Batch 1,000  of  3,088.\n",
            "  Batch 1,050  of  3,088.\n",
            "  Batch 1,100  of  3,088.\n",
            "  Batch 1,150  of  3,088.\n",
            "  Batch 1,200  of  3,088.\n",
            "  Batch 1,250  of  3,088.\n",
            "  Batch 1,300  of  3,088.\n",
            "  Batch 1,350  of  3,088.\n",
            "  Batch 1,400  of  3,088.\n",
            "  Batch 1,450  of  3,088.\n",
            "  Batch 1,500  of  3,088.\n",
            "  Batch 1,550  of  3,088.\n",
            "  Batch 1,600  of  3,088.\n",
            "  Batch 1,650  of  3,088.\n",
            "  Batch 1,700  of  3,088.\n",
            "  Batch 1,750  of  3,088.\n",
            "  Batch 1,800  of  3,088.\n",
            "  Batch 1,850  of  3,088.\n",
            "  Batch 1,900  of  3,088.\n",
            "  Batch 1,950  of  3,088.\n",
            "  Batch 2,000  of  3,088.\n",
            "  Batch 2,050  of  3,088.\n",
            "  Batch 2,100  of  3,088.\n",
            "  Batch 2,150  of  3,088.\n",
            "  Batch 2,200  of  3,088.\n",
            "  Batch 2,250  of  3,088.\n",
            "  Batch 2,300  of  3,088.\n",
            "  Batch 2,350  of  3,088.\n",
            "  Batch 2,400  of  3,088.\n",
            "  Batch 2,450  of  3,088.\n",
            "  Batch 2,500  of  3,088.\n",
            "  Batch 2,550  of  3,088.\n",
            "  Batch 2,600  of  3,088.\n",
            "  Batch 2,650  of  3,088.\n",
            "  Batch 2,700  of  3,088.\n",
            "  Batch 2,750  of  3,088.\n",
            "  Batch 2,800  of  3,088.\n",
            "  Batch 2,850  of  3,088.\n",
            "  Batch 2,900  of  3,088.\n",
            "  Batch 2,950  of  3,088.\n",
            "  Batch 3,000  of  3,088.\n",
            "  Batch 3,050  of  3,088.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    163.\n",
            "  Batch   100  of    163.\n",
            "  Batch   150  of    163.\n",
            "\n",
            "Training Loss: 0.261\n",
            "Validation Loss: nan\n"
          ]
        }
      ],
      "source": [
        "# Train and predict\n",
        "best_valid_loss = float('inf')\n",
        "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(epochs):     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))     \n",
        "    train_loss = train()                       # train model\n",
        "    valid_loss = evaluate()                    # evaluate model\n",
        "    if valid_loss < best_valid_loss:              # save the best model\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'cmtds_new_model_weights.pt')\n",
        "    train_losses.append(train_loss)               # append training and validation loss\n",
        "    valid_losses.append(valid_loss)\n",
        "    scheduler.step()\n",
        "    \n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}') # was sometimes nan for some reasons this problem did not occur with distilbert \n",
        "    #but since other models were not required we decided to add this too we used them later for our ensemble classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "r_31StkcWpeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model performance"
      ],
      "metadata": {
        "id": "Qp7jw9a6qiPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '{}_weights.pt'.format(model_name)\n",
        "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "t7K8z8CVdJup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb2e8e0-db0c-4483-e697-2d962985d434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRgRYquAoWbu"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "preds = []\n",
        "predictions = []\n",
        "for i in range(0, len(test_seq), batch_size):\n",
        "    batch_seq = test_seq[i:i + batch_size]\n",
        "    batch_mask = test_mask[i:i + batch_size]\n",
        "    batch_labels = test_y[i:i + batch_size]\n",
        "    with torch.no_grad():\n",
        "        batch_seq = batch_seq.to(device=device)\n",
        "        batch_mask = batch_mask.to(device=device)\n",
        "        output = model(batch_seq, batch_mask).detach().cpu().numpy()\n",
        "        \n",
        "       \n",
        "        # we take the index of the highest value which represents our class 1-52\n",
        "        # the tensor has size [batchsize (32), number of words in sentence (currently 20), class probabilities(52 of them) ]\n",
        "        predicted = np.argmax(output, axis=-1)\n",
        "       \n",
        "        #predictions.append(predicted)\n",
        "        # each tokenized word has one class it belongs most likely to (an int between 1-52) we take all the values and assign the class label of the sentence to the \n",
        "        # class that was predicted most often  \n",
        "        for j in range(predicted.shape[0]): # 32 times (for each element in batch size )\n",
        "            value = predicted[j]\n",
        "            # we append the class to our predictions \n",
        "            predictions.append(value)\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "      \n",
        "        \n",
        "\n",
        "preds = np.array(predictions)\n",
        "# the argmax of the onehot encoding gives an int 1-52\n",
        "maxs = np.argmax(test_y, axis=1)\n",
        "# save the predictions in a dataframe for comparison between the models \n",
        "pandas_p = pd.DataFrame(preds, columns=[\"prediction\"])\n",
        "pandas_l = pd.DataFrame(maxs, columns=[\"labels\"])\n",
        "pd.DataFrame(pd.concat([pandas_p, pandas_l], axis=1)).to_csv(\"{}our.csv\".format(model_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "# this shows an old confusion matrix of the bert model our dataset\n",
        "confusion = confusion_matrix(preds, maxs)\n",
        "plt.imshow(confusion)\n",
        "plt.colorbar()\n",
        "print(accuracy_score(maxs,preds))\n",
        "print(precision_score(maxs,preds, average='macro'))\n",
        "print(recall_score(maxs,preds, average='macro'))\n",
        "print(f1_score(maxs,preds, average='weighted'))\n"
      ],
      "metadata": {
        "id": "aZGilMun8p1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "4c4df457-282b-447a-ad42-40b5ecfe7bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.79\n",
            "0.40579710144927533\n",
            "0.79\n",
            "0.8405978858115081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "# here are the results of a prior run with bert only trained on english data es expected the accuracy is significantly lower\n",
        "confusion = confusion_matrix(preds, maxs)\n",
        "plt.imshow(confusion)\n",
        "plt.colorbar()\n",
        "print(accuracy_score(maxs,preds))\n",
        "print(precision_score(maxs,preds, average='macro'))\n",
        "print(recall_score(maxs,preds, average='macro'))\n",
        "print(f1_score(maxs,preds, average='weighted'))"
      ],
      "metadata": {
        "id": "jVoafjl319mH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "6dabd6e1-461f-4599-86a3-e4b0b34176b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6903826871657754\n",
            "0.6459670903014009\n",
            "0.6869500826984255\n",
            "0.654350193539542\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ROBERTA "
      ],
      "metadata": {
        "id": "P95XPCMoq8nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '{}1_weights.pt'.format(model_name))"
      ],
      "metadata": {
        "id": "hVr9XJT14dO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fuhUo6BoW-jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GhJJ2TygOzE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXzZXz24PTKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vooQxtj5UBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNbCuFoNipAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20ffdc30026144179acd3e0408bb47eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd7212dec9dc48b6b33461975ac9a97f",
              "IPY_MODEL_829eac4629414e29b03b2d87dc93b76c",
              "IPY_MODEL_e5164128b64e4edd82bf9663bd98c6fd"
            ],
            "layout": "IPY_MODEL_53eb7aa1d0aa4c6c826ab6672c2acf93"
          }
        },
        "fd7212dec9dc48b6b33461975ac9a97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6e1e6453334776815c405373187c26",
            "placeholder": "​",
            "style": "IPY_MODEL_3ad39e8eb5fb4a4c8fa73f81b4e8f5c7",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "829eac4629414e29b03b2d87dc93b76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d12416775424b289f6e0f1cf326672c",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_272f49bed543492e8fc4b3c0667952bf",
            "value": 625
          }
        },
        "e5164128b64e4edd82bf9663bd98c6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc047b2e49d0499ab5bfbf13da8e1d06",
            "placeholder": "​",
            "style": "IPY_MODEL_6fcd9c828830485ab0d45fcf2a7be1f2",
            "value": " 625/625 [00:00&lt;00:00, 6.68kB/s]"
          }
        },
        "53eb7aa1d0aa4c6c826ab6672c2acf93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6e1e6453334776815c405373187c26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad39e8eb5fb4a4c8fa73f81b4e8f5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d12416775424b289f6e0f1cf326672c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272f49bed543492e8fc4b3c0667952bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc047b2e49d0499ab5bfbf13da8e1d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fcd9c828830485ab0d45fcf2a7be1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f337c25e3bb43ec8c5ad523b7e09630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86879b1a27224703bfb37094b88672e8",
              "IPY_MODEL_cecbfedbfe4e4097a8b54466df1975e7",
              "IPY_MODEL_d61b85ff749b4c30bb3216db74511a7c"
            ],
            "layout": "IPY_MODEL_df9ec7c1246a47d592c183e6624cd5e0"
          }
        },
        "86879b1a27224703bfb37094b88672e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2216f3a19d464b97bcf536e9f4f02c",
            "placeholder": "​",
            "style": "IPY_MODEL_25b1439962144c8984930c3ccc9cf6c9",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "cecbfedbfe4e4097a8b54466df1975e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69eec1c2d08e43b1afe78ca0b5b8ddf2",
            "max": 672271273,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f760e8a1d63449b6b0914d2dcc52e989",
            "value": 672271273
          }
        },
        "d61b85ff749b4c30bb3216db74511a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d39169c6967d4be5827e9d5b685d8152",
            "placeholder": "​",
            "style": "IPY_MODEL_3a96212020b147af8125d7eb0f783e77",
            "value": " 672M/672M [00:08&lt;00:00, 122MB/s]"
          }
        },
        "df9ec7c1246a47d592c183e6624cd5e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c2216f3a19d464b97bcf536e9f4f02c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b1439962144c8984930c3ccc9cf6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69eec1c2d08e43b1afe78ca0b5b8ddf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f760e8a1d63449b6b0914d2dcc52e989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d39169c6967d4be5827e9d5b685d8152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a96212020b147af8125d7eb0f783e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac0c469fd1f4df4ba604a5bda0a8532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bca60bcfd3f943f398985388870c2e4d",
              "IPY_MODEL_f8c28e1640f54a8a988bb569e5d96b04",
              "IPY_MODEL_b11c27960e1d442fa870f2203f29ad5f"
            ],
            "layout": "IPY_MODEL_30bbf8ea27014993812a85738bf102fd"
          }
        },
        "bca60bcfd3f943f398985388870c2e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91a006f93914fe8bb6a572187f2b8c1",
            "placeholder": "​",
            "style": "IPY_MODEL_503185f26ab44233a664a9afb05f0d69",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "f8c28e1640f54a8a988bb569e5d96b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b408dbb46541bea2f4bf4d9215fd17",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c552150fc6c42fcb7608efc250a979d",
            "value": 28
          }
        },
        "b11c27960e1d442fa870f2203f29ad5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec15901d7a04a8cbc6d903010603302",
            "placeholder": "​",
            "style": "IPY_MODEL_80ff18e5c638465d8b4dbb5c299dd92a",
            "value": " 28.0/28.0 [00:00&lt;00:00, 371B/s]"
          }
        },
        "30bbf8ea27014993812a85738bf102fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91a006f93914fe8bb6a572187f2b8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "503185f26ab44233a664a9afb05f0d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b408dbb46541bea2f4bf4d9215fd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c552150fc6c42fcb7608efc250a979d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fec15901d7a04a8cbc6d903010603302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ff18e5c638465d8b4dbb5c299dd92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35f8d88bf0f24682849751478c204b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e77fb2067ed449a3ab4e9cc9c6c969e0",
              "IPY_MODEL_e54d43c5dda548449a7125bd3bbde461",
              "IPY_MODEL_2900b6badd334b2c996ff148628e3e68"
            ],
            "layout": "IPY_MODEL_b6d901ae11cd49f28f989ddf8ce12af5"
          }
        },
        "e77fb2067ed449a3ab4e9cc9c6c969e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df54c816e09146168a68e8d8da094fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_735b956dcc6f4657b828f20a3cfe717f",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "e54d43c5dda548449a7125bd3bbde461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8145f43c723543b1a9c89755ae51ad13",
            "max": 871891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3dd5c0337ec45bf897be6bdb2952437",
            "value": 871891
          }
        },
        "2900b6badd334b2c996ff148628e3e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec2be56b197745698e3e800fafcc857a",
            "placeholder": "​",
            "style": "IPY_MODEL_5a27774d34a241ec8a3f7539667bce26",
            "value": " 872k/872k [00:00&lt;00:00, 2.34MB/s]"
          }
        },
        "b6d901ae11cd49f28f989ddf8ce12af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df54c816e09146168a68e8d8da094fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735b956dcc6f4657b828f20a3cfe717f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8145f43c723543b1a9c89755ae51ad13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dd5c0337ec45bf897be6bdb2952437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec2be56b197745698e3e800fafcc857a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a27774d34a241ec8a3f7539667bce26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b961f3045c234bb78a4f6140678fc68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63b95eb2a2894b98ad5e97b0fa415683",
              "IPY_MODEL_2afbbdced5c94aa1a6fb6e80c33c9e74",
              "IPY_MODEL_6a80d09600be4c43809b4edad2d54e9c"
            ],
            "layout": "IPY_MODEL_6b150548ded6416b875b879be2a10150"
          }
        },
        "63b95eb2a2894b98ad5e97b0fa415683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36286784162a482e8f4d12a1bb07836e",
            "placeholder": "​",
            "style": "IPY_MODEL_d3cd0c381f5145a59b2f7c1200434a18",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "2afbbdced5c94aa1a6fb6e80c33c9e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ca178193b5a4af1b83c1e4c4a838c66",
            "max": 1715180,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_679173f824ab453291c8866f5e099a00",
            "value": 1715180
          }
        },
        "6a80d09600be4c43809b4edad2d54e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b518b02f5a340d6baab58001216e774",
            "placeholder": "​",
            "style": "IPY_MODEL_29443d8a3e5c490da8d41379acb63a63",
            "value": " 1.72M/1.72M [00:00&lt;00:00, 5.49MB/s]"
          }
        },
        "6b150548ded6416b875b879be2a10150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36286784162a482e8f4d12a1bb07836e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3cd0c381f5145a59b2f7c1200434a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ca178193b5a4af1b83c1e4c4a838c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "679173f824ab453291c8866f5e099a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b518b02f5a340d6baab58001216e774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29443d8a3e5c490da8d41379acb63a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}